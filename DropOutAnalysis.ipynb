{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c15eec1-c7f5-4ac7-812f-8f0b965cefba",
   "metadata": {},
   "source": [
    "# University Student Dropout Prediction Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bf7a1-6f31-4126-9b2e-912e44642068",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this project, we aim to develop a machine learning model to predict the likelihood of university students dropping out. The challenge of student dropouts is a critical issue in higher education, impacting both the students' future and the educational institutions' effectiveness. Through predictive modeling, we seek to understand the key factors influencing dropout rates and identify at-risk students early in their academic journey.\n",
    "\n",
    "Our methodology is informed by the approach used by Niyogisubizo et al. in their two-layer ensemble machine learning model. However, our focus expands to the broader context of university-wide student retention, rather than individual classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9431d26-b335-4556-996e-5fda6897afd5",
   "metadata": {},
   "source": [
    "### Project Objectives:\n",
    "\n",
    "1. **Data Collection:** Acquire comprehensive and relevant datasets from universities, encompassing various factors like student demographics, academic records, engagement levels, and more.\n",
    "2. **Data Preprocessing:** Clean and preprocess the data to ensure accuracy and reliability for our predictive analysis.\n",
    "3. **Exploratory Data Analysis (EDA):** Perform in-depth analysis to uncover trends and insights within the data, guiding our feature selection and modeling approach.\n",
    "4.  **Development:** Construct a predictive model utilizing mehtods such as Random Forest, XGBoost, Gradient Boosting, and Feed-forward Neural Networks, leveraging their combined strengths.\n",
    "5. **Model Evaluation and Tuning:** Utilize relevant performance metrics to evaluate and refine the model, aiming for enhanced predictive accuracy and robustness.\n",
    "6. **Interpretation and Reporting:** Interpret the results to provide meaningful insights and recommendations, focusing on strategies to improve student retention rates at the university level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d5057-3c68-40c7-8b6e-ff53a819aa86",
   "metadata": {},
   "source": [
    "## 1. Data Collection:\n",
    "\n",
    "**Sources Include:**\n",
    "University requested student drop out data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a65927-53dc-455c-ae28-bd8d71f511bb",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166baa13-8190-4ff4-ab46-1c7a2a764f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8f73ce-82e6-40a9-9c68-55e5876f3505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PIDM</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>SEX</th>\n",
       "      <th>Degree</th>\n",
       "      <th>Major 1</th>\n",
       "      <th>1st Year GPA</th>\n",
       "      <th>Dorm</th>\n",
       "      <th>1st Year Retention</th>\n",
       "      <th>College</th>\n",
       "      <th>Total Earned Hours</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Major 2</th>\n",
       "      <th>Advisor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>2.49</td>\n",
       "      <td>Campion Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>SEC</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>Biology</td>\n",
       "      <td>3.18</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>1</td>\n",
       "      <td>CAS</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Regis Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>CAS</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>DSB Undeclared</td>\n",
       "      <td>3.84</td>\n",
       "      <td>Gonzaga Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>DSB</td>\n",
       "      <td>45</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>Management</td>\n",
       "      <td>2.69</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>1</td>\n",
       "      <td>DSB</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PIDM   Cohort SEX Degree                 Major 1  1st Year GPA  \\\n",
       "0     1  202109F   M     BS  Mechanical Engineering          2.49   \n",
       "1     2  202109F   M     BS                 Biology          3.18   \n",
       "2     3  202109F   M     BS               Chemistry          2.86   \n",
       "3     4  202109F   M     BS          DSB Undeclared          3.84   \n",
       "4     5  202109F   M     BS              Management          2.69   \n",
       "\n",
       "           Dorm  1st Year Retention College  Total Earned Hours     SAT  \\\n",
       "0  Campion Hall                   1     SEC                  36     NaN   \n",
       "1      Commuter                   1     CAS                  47     NaN   \n",
       "2    Regis Hall                   1     CAS                  46     NaN   \n",
       "3  Gonzaga Hall                   1     DSB                  45  1300.0   \n",
       "4      Commuter                   1     DSB                  42     NaN   \n",
       "\n",
       "  Major 2  Advisor  \n",
       "0     NaN      1.0  \n",
       "1     NaN      2.0  \n",
       "2     NaN      3.0  \n",
       "3     NaN      4.0  \n",
       "4     NaN      5.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/kflemming30/Student-Drop-Out-Prediction/main/OIR_Student%20Data%20Request.csv\"\n",
    "student_df = pd.read_csv(url)\n",
    "student_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31c2eba-3e5b-4e86-93d3-f57ab33741f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2584, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dccfac0-f837-42a0-a5fc-d8ab0be3711f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2335\n",
       "0     249\n",
       "Name: 1st Year Retention, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df['1st Year Retention'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3f177a-33c5-4ae9-919d-69c80618d159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PIDM</th>\n",
       "      <th>1st Year GPA</th>\n",
       "      <th>1st Year Retention</th>\n",
       "      <th>Total Earned Hours</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Advisor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2584.000000</td>\n",
       "      <td>2576.000000</td>\n",
       "      <td>2584.000000</td>\n",
       "      <td>2584.000000</td>\n",
       "      <td>632.000000</td>\n",
       "      <td>2576.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1292.500000</td>\n",
       "      <td>3.360839</td>\n",
       "      <td>0.903638</td>\n",
       "      <td>45.852167</td>\n",
       "      <td>1306.977848</td>\n",
       "      <td>56.611413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>746.080871</td>\n",
       "      <td>0.549521</td>\n",
       "      <td>0.295144</td>\n",
       "      <td>10.109841</td>\n",
       "      <td>88.935666</td>\n",
       "      <td>47.163027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>980.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>646.750000</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1292.500000</td>\n",
       "      <td>3.490000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1310.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1938.250000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1370.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2584.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1550.000000</td>\n",
       "      <td>166.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              PIDM  1st Year GPA  1st Year Retention  Total Earned Hours  \\\n",
       "count  2584.000000   2576.000000         2584.000000         2584.000000   \n",
       "mean   1292.500000      3.360839            0.903638           45.852167   \n",
       "std     746.080871      0.549521            0.295144           10.109841   \n",
       "min       1.000000      0.000000            0.000000            0.000000   \n",
       "25%     646.750000      3.110000            1.000000           45.000000   \n",
       "50%    1292.500000      3.490000            1.000000           46.000000   \n",
       "75%    1938.250000      3.750000            1.000000           51.000000   \n",
       "max    2584.000000      4.000000            1.000000           81.000000   \n",
       "\n",
       "               SAT      Advisor  \n",
       "count   632.000000  2576.000000  \n",
       "mean   1306.977848    56.611413  \n",
       "std      88.935666    47.163027  \n",
       "min     980.000000     0.000000  \n",
       "25%    1240.000000    11.000000  \n",
       "50%    1310.000000    53.000000  \n",
       "75%    1370.000000    88.000000  \n",
       "max    1550.000000   166.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "288e4d07-a5d8-4803-803b-ee2096103ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2584 entries, 0 to 2583\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   PIDM                2584 non-null   int64  \n",
      " 1   Cohort              2584 non-null   object \n",
      " 2   SEX                 2584 non-null   object \n",
      " 3   Degree              2584 non-null   object \n",
      " 4   Major 1             2584 non-null   object \n",
      " 5   1st Year GPA        2576 non-null   float64\n",
      " 6   Dorm                2584 non-null   object \n",
      " 7   1st Year Retention  2584 non-null   int64  \n",
      " 8   College             2584 non-null   object \n",
      " 9   Total Earned Hours  2584 non-null   int64  \n",
      " 10  SAT                 632 non-null    float64\n",
      " 11  Major 2             6 non-null      object \n",
      " 12  Advisor             2576 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(7)\n",
      "memory usage: 262.6+ KB\n"
     ]
    }
   ],
   "source": [
    "student_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47465b6d-f8db-420e-bec6-c82027682683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PIDM</th>\n",
       "      <th>1st Year GPA</th>\n",
       "      <th>1st Year Retention</th>\n",
       "      <th>Total Earned Hours</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Major 2</th>\n",
       "      <th>Advisor</th>\n",
       "      <th>Cohort_202109F</th>\n",
       "      <th>Cohort_202209F</th>\n",
       "      <th>SEX_F</th>\n",
       "      <th>...</th>\n",
       "      <th>Dorm_Claver Hall</th>\n",
       "      <th>Dorm_Commuter</th>\n",
       "      <th>Dorm_Gonzaga Hall</th>\n",
       "      <th>Dorm_Jogues Hall</th>\n",
       "      <th>Dorm_Loyola Hall</th>\n",
       "      <th>Dorm_Regis Hall</th>\n",
       "      <th>College_CAS</th>\n",
       "      <th>College_DSB</th>\n",
       "      <th>College_EGAN</th>\n",
       "      <th>College_SEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.18</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PIDM  1st Year GPA  1st Year Retention  Total Earned Hours     SAT Major 2  \\\n",
       "0     1          2.49                   1                  36     NaN     NaN   \n",
       "1     2          3.18                   1                  47     NaN     NaN   \n",
       "2     3          2.86                   1                  46     NaN     NaN   \n",
       "3     4          3.84                   1                  45  1300.0     NaN   \n",
       "4     5          2.69                   1                  42     NaN     NaN   \n",
       "\n",
       "   Advisor  Cohort_202109F  Cohort_202209F  SEX_F  ...  Dorm_Claver Hall  \\\n",
       "0      1.0               1               0      0  ...                 0   \n",
       "1      2.0               1               0      0  ...                 0   \n",
       "2      3.0               1               0      0  ...                 0   \n",
       "3      4.0               1               0      0  ...                 0   \n",
       "4      5.0               1               0      0  ...                 0   \n",
       "\n",
       "   Dorm_Commuter  Dorm_Gonzaga Hall  Dorm_Jogues Hall  Dorm_Loyola Hall  \\\n",
       "0              0                  0                 0                 0   \n",
       "1              1                  0                 0                 0   \n",
       "2              0                  0                 0                 0   \n",
       "3              0                  1                 0                 0   \n",
       "4              1                  0                 0                 0   \n",
       "\n",
       "   Dorm_Regis Hall  College_CAS  College_DSB  College_EGAN  College_SEC  \n",
       "0                0            0            0             0            1  \n",
       "1                0            1            0             0            0  \n",
       "2                1            1            0             0            0  \n",
       "3                0            0            1             0            0  \n",
       "4                0            0            1             0            0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Select categorical columns for one-hot encoding\n",
    "categorical_cols = ['Cohort', 'SEX', 'Degree', 'Major 1', 'Dorm', 'College']\n",
    "\n",
    "# Perform one-hot encoding\n",
    "student_df_encoded = pd.get_dummies(student_df, columns=categorical_cols)\n",
    "\n",
    "student_df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5aa785c-26a2-41eb-8902-c46e3c97c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2584 entries, 0 to 2583\n",
      "Data columns (total 62 columns):\n",
      " #   Column                                       Non-Null Count  Dtype  \n",
      "---  ------                                       --------------  -----  \n",
      " 0   PIDM                                         2584 non-null   int64  \n",
      " 1   1st Year GPA                                 2576 non-null   float64\n",
      " 2   1st Year Retention                           2584 non-null   int64  \n",
      " 3   Total Earned Hours                           2584 non-null   int64  \n",
      " 4   SAT                                          632 non-null    float64\n",
      " 5   Major 2                                      6 non-null      object \n",
      " 6   Advisor                                      2576 non-null   float64\n",
      " 7   Cohort_202109F                               2584 non-null   uint8  \n",
      " 8   Cohort_202209F                               2584 non-null   uint8  \n",
      " 9   SEX_F                                        2584 non-null   uint8  \n",
      " 10  SEX_M                                        2584 non-null   uint8  \n",
      " 11  Degree_BA                                    2584 non-null   uint8  \n",
      " 12  Degree_BS                                    2584 non-null   uint8  \n",
      " 13  Degree_BSW                                   2584 non-null   uint8  \n",
      " 14  Major 1_Accounting                           2584 non-null   uint8  \n",
      " 15  Major 1_American Studies                     2584 non-null   uint8  \n",
      " 16  Major 1_Biology                              2584 non-null   uint8  \n",
      " 17  Major 1_Biomedical Engineering               2584 non-null   uint8  \n",
      " 18  Major 1_Business Analytics                   2584 non-null   uint8  \n",
      " 19  Major 1_Chemistry                            2584 non-null   uint8  \n",
      " 20  Major 1_Communication                        2584 non-null   uint8  \n",
      " 21  Major 1_Computer Science                     2584 non-null   uint8  \n",
      " 22  Major 1_DSB Undeclared                       2584 non-null   uint8  \n",
      " 23  Major 1_Digital Journalism                   2584 non-null   uint8  \n",
      " 24  Major 1_Economics                            2584 non-null   uint8  \n",
      " 25  Major 1_Electrical and Computer Engineering  2584 non-null   uint8  \n",
      " 26  Major 1_English                              2584 non-null   uint8  \n",
      " 27  Major 1_Finance                              2584 non-null   uint8  \n",
      " 28  Major 1_History                              2584 non-null   uint8  \n",
      " 29  Major 1_Information Systems & Ops Mgmt       2584 non-null   uint8  \n",
      " 30  Major 1_International Business               2584 non-null   uint8  \n",
      " 31  Major 1_International Studies                2584 non-null   uint8  \n",
      " 32  Major 1_Management                           2584 non-null   uint8  \n",
      " 33  Major 1_Marketing                            2584 non-null   uint8  \n",
      " 34  Major 1_Mathematics                          2584 non-null   uint8  \n",
      " 35  Major 1_Mechanical Engineering               2584 non-null   uint8  \n",
      " 36  Major 1_Modern Languages                     2584 non-null   uint8  \n",
      " 37  Major 1_Nursing                              2584 non-null   uint8  \n",
      " 38  Major 1_Physics                              2584 non-null   uint8  \n",
      " 39  Major 1_Politics                             2584 non-null   uint8  \n",
      " 40  Major 1_Program on the Environment           2584 non-null   uint8  \n",
      " 41  Major 1_Psychology                           2584 non-null   uint8  \n",
      " 42  Major 1_Public Health                        2584 non-null   uint8  \n",
      " 43  Major 1_Religious Studies                    2584 non-null   uint8  \n",
      " 44  Major 1_SOE Undeclared                       2584 non-null   uint8  \n",
      " 45  Major 1_Social Work                          2584 non-null   uint8  \n",
      " 46  Major 1_Sociology and Anthropology           2584 non-null   uint8  \n",
      " 47  Major 1_Sports Media                         2584 non-null   uint8  \n",
      " 48  Major 1_Undeclared                           2584 non-null   uint8  \n",
      " 49  Major 1_Visual & Performing Arts             2584 non-null   uint8  \n",
      " 50  Dorm_1036 North Benson Road                  2584 non-null   uint8  \n",
      " 51  Dorm_Campion Hall                            2584 non-null   uint8  \n",
      " 52  Dorm_Claver Hall                             2584 non-null   uint8  \n",
      " 53  Dorm_Commuter                                2584 non-null   uint8  \n",
      " 54  Dorm_Gonzaga Hall                            2584 non-null   uint8  \n",
      " 55  Dorm_Jogues Hall                             2584 non-null   uint8  \n",
      " 56  Dorm_Loyola Hall                             2584 non-null   uint8  \n",
      " 57  Dorm_Regis Hall                              2584 non-null   uint8  \n",
      " 58  College_CAS                                  2584 non-null   uint8  \n",
      " 59  College_DSB                                  2584 non-null   uint8  \n",
      " 60  College_EGAN                                 2584 non-null   uint8  \n",
      " 61  College_SEC                                  2584 non-null   uint8  \n",
      "dtypes: float64(3), int64(3), object(1), uint8(55)\n",
      "memory usage: 280.2+ KB\n"
     ]
    }
   ],
   "source": [
    "student_df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed39ce8e-8b9b-4f38-a50f-d1d873bea069",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df_encoded = student_df_encoded.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07baa4c7-1c4b-44d5-8e1d-c843e4fdc6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2584 entries, 0 to 2583\n",
      "Data columns (total 58 columns):\n",
      " #   Column                                       Non-Null Count  Dtype\n",
      "---  ------                                       --------------  -----\n",
      " 0   PIDM                                         2584 non-null   int64\n",
      " 1   1st Year Retention                           2584 non-null   int64\n",
      " 2   Total Earned Hours                           2584 non-null   int64\n",
      " 3   Cohort_202109F                               2584 non-null   uint8\n",
      " 4   Cohort_202209F                               2584 non-null   uint8\n",
      " 5   SEX_F                                        2584 non-null   uint8\n",
      " 6   SEX_M                                        2584 non-null   uint8\n",
      " 7   Degree_BA                                    2584 non-null   uint8\n",
      " 8   Degree_BS                                    2584 non-null   uint8\n",
      " 9   Degree_BSW                                   2584 non-null   uint8\n",
      " 10  Major 1_Accounting                           2584 non-null   uint8\n",
      " 11  Major 1_American Studies                     2584 non-null   uint8\n",
      " 12  Major 1_Biology                              2584 non-null   uint8\n",
      " 13  Major 1_Biomedical Engineering               2584 non-null   uint8\n",
      " 14  Major 1_Business Analytics                   2584 non-null   uint8\n",
      " 15  Major 1_Chemistry                            2584 non-null   uint8\n",
      " 16  Major 1_Communication                        2584 non-null   uint8\n",
      " 17  Major 1_Computer Science                     2584 non-null   uint8\n",
      " 18  Major 1_DSB Undeclared                       2584 non-null   uint8\n",
      " 19  Major 1_Digital Journalism                   2584 non-null   uint8\n",
      " 20  Major 1_Economics                            2584 non-null   uint8\n",
      " 21  Major 1_Electrical and Computer Engineering  2584 non-null   uint8\n",
      " 22  Major 1_English                              2584 non-null   uint8\n",
      " 23  Major 1_Finance                              2584 non-null   uint8\n",
      " 24  Major 1_History                              2584 non-null   uint8\n",
      " 25  Major 1_Information Systems & Ops Mgmt       2584 non-null   uint8\n",
      " 26  Major 1_International Business               2584 non-null   uint8\n",
      " 27  Major 1_International Studies                2584 non-null   uint8\n",
      " 28  Major 1_Management                           2584 non-null   uint8\n",
      " 29  Major 1_Marketing                            2584 non-null   uint8\n",
      " 30  Major 1_Mathematics                          2584 non-null   uint8\n",
      " 31  Major 1_Mechanical Engineering               2584 non-null   uint8\n",
      " 32  Major 1_Modern Languages                     2584 non-null   uint8\n",
      " 33  Major 1_Nursing                              2584 non-null   uint8\n",
      " 34  Major 1_Physics                              2584 non-null   uint8\n",
      " 35  Major 1_Politics                             2584 non-null   uint8\n",
      " 36  Major 1_Program on the Environment           2584 non-null   uint8\n",
      " 37  Major 1_Psychology                           2584 non-null   uint8\n",
      " 38  Major 1_Public Health                        2584 non-null   uint8\n",
      " 39  Major 1_Religious Studies                    2584 non-null   uint8\n",
      " 40  Major 1_SOE Undeclared                       2584 non-null   uint8\n",
      " 41  Major 1_Social Work                          2584 non-null   uint8\n",
      " 42  Major 1_Sociology and Anthropology           2584 non-null   uint8\n",
      " 43  Major 1_Sports Media                         2584 non-null   uint8\n",
      " 44  Major 1_Undeclared                           2584 non-null   uint8\n",
      " 45  Major 1_Visual & Performing Arts             2584 non-null   uint8\n",
      " 46  Dorm_1036 North Benson Road                  2584 non-null   uint8\n",
      " 47  Dorm_Campion Hall                            2584 non-null   uint8\n",
      " 48  Dorm_Claver Hall                             2584 non-null   uint8\n",
      " 49  Dorm_Commuter                                2584 non-null   uint8\n",
      " 50  Dorm_Gonzaga Hall                            2584 non-null   uint8\n",
      " 51  Dorm_Jogues Hall                             2584 non-null   uint8\n",
      " 52  Dorm_Loyola Hall                             2584 non-null   uint8\n",
      " 53  Dorm_Regis Hall                              2584 non-null   uint8\n",
      " 54  College_CAS                                  2584 non-null   uint8\n",
      " 55  College_DSB                                  2584 non-null   uint8\n",
      " 56  College_EGAN                                 2584 non-null   uint8\n",
      " 57  College_SEC                                  2584 non-null   uint8\n",
      "dtypes: int64(3), uint8(55)\n",
      "memory usage: 199.5 KB\n"
     ]
    }
   ],
   "source": [
    "student_df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c61839-0c40-4d50-89f3-4b21000a08af",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32055311-4f34-49ca-a443-b21dad274302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2335\n",
       "0     249\n",
       "Name: 1st Year Retention, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df_encoded['1st Year Retention'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf960f9-368c-47f8-9eb7-a117b1c2860d",
   "metadata": {},
   "source": [
    "## 4. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92ba14d6-eb4e-45b0-98c1-085d8a541268",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = student_df_encoded.columns[(student_df_encoded.columns != 'PIDM') & (student_df_encoded.columns != '1st Year Retention')]\n",
    "target = \"1st Year Retention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "130cc9e8-4c3e-42b6-89f4-2a225195ad4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2067, 56), (2067,), (517, 56), (517,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = student_df_encoded[features]\n",
    "y = student_df_encoded[target]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4c56b8f-b28f-43e0-b39e-b64efb7ad1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_test2(X_train, X_test, y_train, y_test, param_grid, clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"*** Parameter estimation results: \")\n",
    "    print(clf.cv_results_)\n",
    "    print()\n",
    "\n",
    "    print(\"*** Grid scores: \")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "\n",
    "    for mean, std, param in zip(means, stds, params):\n",
    "        print(f\"{round(mean, 3)} (+/-{round(std*2, 3)}) for {param}\")\n",
    "    print()\n",
    "\n",
    "    print(\"*** Highest accuracy score: \")\n",
    "    print(f\"{round(clf.best_score_, 3)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"*** Best parameters set found: \")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "    print(\"*** Classification report for the best parameters set: \")\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    print(\"*** Confusion matrix for the best parameters set: \")\n",
    "    print(metrics.confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    print(\"*** Final accuracy score: \")\n",
    "    test_score = round(clf.score(X_test, y_test), 3)\n",
    "    print(test_score)\n",
    "\n",
    "    return clf, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5498129d-903c-4492-93e5-86657c782da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "summary = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fee27-11a6-4513-87d8-49797f80460a",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15bc9fc9-94d7-4104-a35c-69318d534c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\"n_neighbors\": [1, 3, 10, 30, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec9c7930-9ff4-4f7c-af02-575a2f6fdad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid=[{&#x27;n_neighbors&#x27;: [1, 3, 10, 30, 100]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid=[{&#x27;n_neighbors&#x27;: [1, 3, 10, 30, 100]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid=[{'n_neighbors': [1, 3, 10, 30, 100]}])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knc = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cv)\n",
    "knc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c60d1d5f-3345-43a3-8658-f8d7d791f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.00243521, 0.00223074, 0.0020205 , 0.00160122, 0.00220819]), 'std_fit_time': array([7.87534685e-04, 3.45970599e-04, 1.99814684e-05, 4.92567730e-04,\n",
      "       4.16068890e-04]), 'mean_score_time': array([0.03127427, 0.01286249, 0.01755128, 0.01739321, 0.01890998]), 'std_score_time': array([0.03773044, 0.00069117, 0.00020119, 0.00036096, 0.00026017]), 'param_n_neighbors': masked_array(data=[1, 3, 10, 30, 100],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 10}, {'n_neighbors': 30}, {'n_neighbors': 100}], 'split0_test_score': array([0.96135266, 0.98309179, 0.98067633, 0.98550725, 0.98550725]), 'split1_test_score': array([0.96376812, 0.97342995, 0.97826087, 0.97826087, 0.97826087]), 'split2_test_score': array([0.97578692, 0.98547215, 0.98789346, 0.98547215, 0.98547215]), 'split3_test_score': array([0.95883777, 0.968523  , 0.968523  , 0.968523  , 0.968523  ]), 'split4_test_score': array([0.96610169, 0.97578692, 0.97578692, 0.97578692, 0.97578692]), 'mean_test_score': array([0.96516943, 0.97726076, 0.97822812, 0.97871004, 0.97871004]), 'std_test_score': array([0.00583475, 0.00623897, 0.00631763, 0.00639453, 0.00639453]), 'rank_test_score': array([5, 4, 3, 1, 1])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.965 (+/-0.012) for {'n_neighbors': 1}\n",
      "0.977 (+/-0.012) for {'n_neighbors': 3}\n",
      "0.978 (+/-0.013) for {'n_neighbors': 10}\n",
      "0.979 (+/-0.013) for {'n_neighbors': 30}\n",
      "0.979 (+/-0.013) for {'n_neighbors': 100}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.979\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'n_neighbors': 30}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        55\n",
      "           1       0.99      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.94      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 49   6]\n",
      " [  8 454]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    }
   ],
   "source": [
    "knc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, knc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f2cae2a-9544-46e3-84d9-902371b4c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[\"k-NNs\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4b04b-d9de-417a-ade5-188697432b81",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61408d38-cd9d-450f-83da-55ab914c16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.0215632 , 0.00390429, 0.01907907, 0.00313249, 0.03486347,\n",
      "       0.00342536, 0.04339461, 0.0044951 , 0.03844342, 0.00390282,\n",
      "       0.03828588, 0.00427518, 0.04030013, 0.00479379]), 'std_fit_time': array([9.22369786e-03, 1.56588313e-03, 2.52038365e-03, 4.72469834e-05,\n",
      "       2.04485439e-02, 4.67491962e-04, 7.58949490e-03, 4.69298940e-04,\n",
      "       5.64246610e-03, 4.57270565e-04, 4.42928458e-03, 5.54292926e-04,\n",
      "       3.00795538e-03, 3.86780152e-04]), 'mean_score_time': array([0.00105004, 0.00126553, 0.00123997, 0.00170135, 0.00180221,\n",
      "       0.0015029 , 0.00200558, 0.00183668, 0.00175686, 0.00140724,\n",
      "       0.00225692, 0.0012094 , 0.00175066, 0.00160408]), 'std_score_time': array([8.13531427e-05, 4.88019455e-04, 7.47397510e-04, 5.60226904e-04,\n",
      "       3.96371042e-04, 4.34585802e-04, 5.13126413e-06, 3.50985532e-04,\n",
      "       4.60850555e-04, 4.52566185e-04, 3.80552184e-04, 3.76175368e-04,\n",
      "       6.08562634e-04, 4.92353081e-04]), 'param_C': masked_array(data=[0.01, 0.01, 0.03, 0.03, 0.1, 0.1, 0.3, 0.3, 1, 1, 3, 3,\n",
      "                   10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_solver': masked_array(data=['lbfgs', 'liblinear', 'lbfgs', 'liblinear', 'lbfgs',\n",
      "                   'liblinear', 'lbfgs', 'liblinear', 'lbfgs',\n",
      "                   'liblinear', 'lbfgs', 'liblinear', 'lbfgs',\n",
      "                   'liblinear'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01, 'solver': 'lbfgs'}, {'C': 0.01, 'solver': 'liblinear'}, {'C': 0.03, 'solver': 'lbfgs'}, {'C': 0.03, 'solver': 'liblinear'}, {'C': 0.1, 'solver': 'lbfgs'}, {'C': 0.1, 'solver': 'liblinear'}, {'C': 0.3, 'solver': 'lbfgs'}, {'C': 0.3, 'solver': 'liblinear'}, {'C': 1, 'solver': 'lbfgs'}, {'C': 1, 'solver': 'liblinear'}, {'C': 3, 'solver': 'lbfgs'}, {'C': 3, 'solver': 'liblinear'}, {'C': 10, 'solver': 'lbfgs'}, {'C': 10, 'solver': 'liblinear'}], 'split0_test_score': array([0.97826087, 0.92270531, 0.97826087, 0.93961353, 0.98067633,\n",
      "       0.94202899, 0.98067633, 0.96135266, 0.97826087, 0.96618357,\n",
      "       0.96859903, 0.97101449, 0.96618357, 0.97101449]), 'split1_test_score': array([0.97584541, 0.92512077, 0.97826087, 0.94444444, 0.98067633,\n",
      "       0.9468599 , 0.97826087, 0.96376812, 0.97584541, 0.97101449,\n",
      "       0.97342995, 0.97342995, 0.97342995, 0.97342995]), 'split2_test_score': array([0.98305085, 0.93220339, 0.98305085, 0.94673123, 0.98305085,\n",
      "       0.95883777, 0.98062954, 0.97094431, 0.98305085, 0.97336562,\n",
      "       0.97820823, 0.97820823, 0.97820823, 0.97820823]), 'split3_test_score': array([0.96368039, 0.91283293, 0.96610169, 0.93220339, 0.96610169,\n",
      "       0.92736077, 0.968523  , 0.94673123, 0.96368039, 0.96125908,\n",
      "       0.96368039, 0.96368039, 0.96368039, 0.96368039]), 'split4_test_score': array([0.97336562, 0.9346247 , 0.97094431, 0.94188862, 0.97094431,\n",
      "       0.94430993, 0.97336562, 0.96610169, 0.97336562, 0.968523  ,\n",
      "       0.97336562, 0.97336562, 0.97336562, 0.97336562]), 'mean_test_score': array([0.97484063, 0.92549742, 0.97532372, 0.94097624, 0.9762899 ,\n",
      "       0.94387947, 0.97629107, 0.9617796 , 0.97484063, 0.96806915,\n",
      "       0.97145664, 0.97193974, 0.97097355, 0.97193974]), 'std_test_score': array([0.00643183, 0.007702  , 0.00601975, 0.00499601, 0.00658129,\n",
      "       0.0100907 , 0.00470816, 0.00816409, 0.00643183, 0.00416812,\n",
      "       0.00493474, 0.00474596, 0.00529586, 0.00474596]), 'rank_test_score': array([ 4, 14,  3, 13,  2, 12,  1, 11,  4, 10,  8,  6,  9,  6])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.975 (+/-0.013) for {'C': 0.01, 'solver': 'lbfgs'}\n",
      "0.925 (+/-0.015) for {'C': 0.01, 'solver': 'liblinear'}\n",
      "0.975 (+/-0.012) for {'C': 0.03, 'solver': 'lbfgs'}\n",
      "0.941 (+/-0.01) for {'C': 0.03, 'solver': 'liblinear'}\n",
      "0.976 (+/-0.013) for {'C': 0.1, 'solver': 'lbfgs'}\n",
      "0.944 (+/-0.02) for {'C': 0.1, 'solver': 'liblinear'}\n",
      "0.976 (+/-0.009) for {'C': 0.3, 'solver': 'lbfgs'}\n",
      "0.962 (+/-0.016) for {'C': 0.3, 'solver': 'liblinear'}\n",
      "0.975 (+/-0.013) for {'C': 1, 'solver': 'lbfgs'}\n",
      "0.968 (+/-0.008) for {'C': 1, 'solver': 'liblinear'}\n",
      "0.971 (+/-0.01) for {'C': 3, 'solver': 'lbfgs'}\n",
      "0.972 (+/-0.009) for {'C': 3, 'solver': 'liblinear'}\n",
      "0.971 (+/-0.011) for {'C': 10, 'solver': 'lbfgs'}\n",
      "0.972 (+/-0.009) for {'C': 10, 'solver': 'liblinear'}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.976\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'C': 0.3, 'solver': 'lbfgs'}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        55\n",
      "           1       0.98      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.93      0.93      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 48   7]\n",
      " [  7 455]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973, 'Logistic Regression': 0.973}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"C\": [0.01, 0.03, 0.1, 0.3, 1, 3, 10], \"solver\": [\"lbfgs\", \"liblinear\"]}]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "lr, score = train_test2(X_train, X_test, y_train, y_test, param_grid, lr)\n",
    "summary[\"Logistic Regression\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ebf5f-ec6c-4c46-88c7-9826f8cd041e",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3f37fed-68c3-4c6f-af09-845f6fec303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.0026176 , 0.0021596 , 0.00403996, 0.00420666]), 'std_fit_time': array([0.00078758, 0.00026445, 0.00061224, 0.00016043]), 'mean_score_time': array([0.00118637, 0.00084114, 0.00124307, 0.00124216]), 'std_score_time': array([0.00040843, 0.00031782, 0.00040774, 0.00043249]), 'param_max_depth': masked_array(data=[1, 3, 10, None],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 1}, {'max_depth': 3}, {'max_depth': 10}, {'max_depth': None}], 'split0_test_score': array([0.98309179, 0.98067633, 0.96859903, 0.96376812]), 'split1_test_score': array([0.97826087, 0.96618357, 0.96135266, 0.96135266]), 'split2_test_score': array([0.98547215, 0.98062954, 0.97578692, 0.96368039]), 'split3_test_score': array([0.96610169, 0.96610169, 0.95641646, 0.95399516]), 'split4_test_score': array([0.97578692, 0.968523  , 0.94430993, 0.94915254]), 'mean_test_score': array([0.97774269, 0.97242283, 0.961293  , 0.95838977]), 'std_test_score': array([0.00675224, 0.0067759 , 0.01072997, 0.00583669]), 'rank_test_score': array([1, 2, 3, 4])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.978 (+/-0.014) for {'max_depth': 1}\n",
      "0.972 (+/-0.014) for {'max_depth': 3}\n",
      "0.961 (+/-0.021) for {'max_depth': 10}\n",
      "0.958 (+/-0.012) for {'max_depth': None}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.978\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'max_depth': 1}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        55\n",
      "           1       0.99      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.94      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 49   6]\n",
      " [  8 454]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973, 'Logistic Regression': 0.973, 'Decision Trees': 0.973}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"max_depth\": [1, 3, 10, None]}]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=cv)\n",
    "dtc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, dtc)\n",
    "summary[\"Decision Trees\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1c1c2-d0d8-4a72-a2ee-eb4654d5d9f0",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bdfd200-b957-42a4-814c-f961754b1dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.00243082, 0.00404325, 0.01063366, 0.02723708, 0.08142266,\n",
      "       0.24292064, 0.89854703, 0.00383501, 0.00630159, 0.01318731,\n",
      "       0.035848  , 0.10382023, 0.26925354, 0.83909163, 0.00324631,\n",
      "       0.0052053 , 0.01457281, 0.03887706, 0.12820635, 0.38322115,\n",
      "       1.412216  , 0.00422063, 0.00789852, 0.0179563 , 0.05732584,\n",
      "       0.15936494, 0.44913168, 1.45396571]), 'std_fit_time': array([0.0004871 , 0.00038824, 0.00104991, 0.00193204, 0.00311806,\n",
      "       0.0111895 , 0.04850224, 0.00042067, 0.00076142, 0.00072632,\n",
      "       0.00601859, 0.0047691 , 0.02361128, 0.03612725, 0.00071353,\n",
      "       0.00012275, 0.00099274, 0.00266393, 0.01058599, 0.01520361,\n",
      "       0.12264897, 0.0009483 , 0.00105364, 0.00102578, 0.00491704,\n",
      "       0.01025284, 0.01686361, 0.05550145]), 'mean_score_time': array([0.0014513 , 0.00131979, 0.00171824, 0.00249515, 0.0050158 ,\n",
      "       0.0133388 , 0.04624476, 0.00188265, 0.00202985, 0.00242939,\n",
      "       0.00332813, 0.00684423, 0.01463027, 0.04217482, 0.00114608,\n",
      "       0.00162005, 0.00192709, 0.00335298, 0.0066854 , 0.01672788,\n",
      "       0.05983844, 0.0018621 , 0.00168223, 0.00259695, 0.00462303,\n",
      "       0.00830932, 0.01827884, 0.05935822]), 'std_score_time': array([5.05053785e-04, 4.86668460e-04, 5.01400437e-04, 5.30302384e-04,\n",
      "       4.44727523e-04, 1.15378187e-03, 5.50491397e-03, 2.04145376e-04,\n",
      "       3.07825624e-05, 4.81693373e-04, 4.50275447e-04, 1.08396275e-03,\n",
      "       2.50911576e-03, 3.41453135e-03, 4.78377144e-04, 5.10336804e-04,\n",
      "       3.40196208e-04, 4.25887598e-04, 5.56273017e-04, 1.04721629e-03,\n",
      "       5.15423044e-03, 8.51873479e-04, 8.09776192e-04, 4.90322879e-04,\n",
      "       9.91969907e-04, 1.59163638e-03, 1.12636098e-03, 4.88788223e-03]), 'param_max_depth': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 10, 10, 10,\n",
      "                   10, 10, 10, 10, None, None, None, None, None, None,\n",
      "                   None],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[1, 3, 10, 30, 100, 300, 1000, 1, 3, 10, 30, 100, 300,\n",
      "                   1000, 1, 3, 10, 30, 100, 300, 1000, 1, 3, 10, 30, 100,\n",
      "                   300, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 1, 'n_estimators': 1}, {'max_depth': 1, 'n_estimators': 3}, {'max_depth': 1, 'n_estimators': 10}, {'max_depth': 1, 'n_estimators': 30}, {'max_depth': 1, 'n_estimators': 100}, {'max_depth': 1, 'n_estimators': 300}, {'max_depth': 1, 'n_estimators': 1000}, {'max_depth': 3, 'n_estimators': 1}, {'max_depth': 3, 'n_estimators': 3}, {'max_depth': 3, 'n_estimators': 10}, {'max_depth': 3, 'n_estimators': 30}, {'max_depth': 3, 'n_estimators': 100}, {'max_depth': 3, 'n_estimators': 300}, {'max_depth': 3, 'n_estimators': 1000}, {'max_depth': 10, 'n_estimators': 1}, {'max_depth': 10, 'n_estimators': 3}, {'max_depth': 10, 'n_estimators': 10}, {'max_depth': 10, 'n_estimators': 30}, {'max_depth': 10, 'n_estimators': 100}, {'max_depth': 10, 'n_estimators': 300}, {'max_depth': 10, 'n_estimators': 1000}, {'max_depth': None, 'n_estimators': 1}, {'max_depth': None, 'n_estimators': 3}, {'max_depth': None, 'n_estimators': 10}, {'max_depth': None, 'n_estimators': 30}, {'max_depth': None, 'n_estimators': 100}, {'max_depth': None, 'n_estimators': 300}, {'max_depth': None, 'n_estimators': 1000}], 'split0_test_score': array([0.89613527, 0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.98550725, 0.98550725, 0.90821256,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.97826087,\n",
      "       0.96618357, 0.96859903, 0.97342995, 0.97826087, 0.98067633,\n",
      "       0.98309179, 0.96376812, 0.97101449, 0.97826087, 0.97584541,\n",
      "       0.97342995, 0.97584541, 0.97584541]), 'split1_test_score': array([0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.92028986, 0.91545894,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.97101449,\n",
      "       0.97826087, 0.97342995, 0.97826087, 0.98067633, 0.97826087,\n",
      "       0.98067633, 0.93719807, 0.96135266, 0.97342995, 0.97342995,\n",
      "       0.97342995, 0.97342995, 0.97584541]), 'split2_test_score': array([0.90799031, 0.90799031, 0.90799031, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.90799031, 0.98547215, 0.98547215, 0.91767554,\n",
      "       0.90799031, 0.90799031, 0.90799031, 0.90799031, 0.97578692,\n",
      "       0.97820823, 0.97820823, 0.97820823, 0.98062954, 0.97820823,\n",
      "       0.97820823, 0.96368039, 0.968523  , 0.97094431, 0.97578692,\n",
      "       0.98305085, 0.98062954, 0.98062954]), 'split3_test_score': array([0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.95883777, 0.96125908, 0.95883777,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.95641646,\n",
      "       0.96368039, 0.96610169, 0.968523  , 0.96610169, 0.96610169,\n",
      "       0.968523  , 0.95399516, 0.96125908, 0.96610169, 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.968523  ]), 'split4_test_score': array([0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.97336562, 0.97336562, 0.90799031,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.96125908,\n",
      "       0.97336562, 0.97578692, 0.97578692, 0.97094431, 0.97336562,\n",
      "       0.968523  , 0.96125908, 0.968523  , 0.97336562, 0.96610169,\n",
      "       0.96610169, 0.96610169, 0.96610169]), 'mean_test_score': array([0.90421214, 0.90614451, 0.90614451, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.90614451, 0.96179598, 0.96517879, 0.92163503,\n",
      "       0.90614451, 0.90614451, 0.90614451, 0.90614451, 0.96854757,\n",
      "       0.97193974, 0.97242517, 0.9748418 , 0.97532255, 0.97532255,\n",
      "       0.97580447, 0.95598016, 0.96613445, 0.97242049, 0.9719374 ,\n",
      "       0.97290709, 0.97290592, 0.97338901]), 'std_test_score': array([0.00414016, 0.00092852, 0.00092852, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00092852, 0.02966907, 0.02417632, 0.01899582,\n",
      "       0.00092852, 0.00092852, 0.00092852, 0.00092852, 0.0084039 ,\n",
      "       0.00604374, 0.00448063, 0.00362868, 0.00582501, 0.00518609,\n",
      "       0.0061426 , 0.0100451 , 0.00404623, 0.00395294, 0.00395051,\n",
      "       0.00581337, 0.00517836, 0.00531556]), 'rank_test_score': array([28, 18, 18, 18, 18, 18, 18, 15, 14, 17, 18, 18, 18, 18, 12, 10,  8,\n",
      "        4,  2,  2,  1, 16, 13,  9, 11,  6,  7,  5])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.904 (+/-0.008) for {'max_depth': 1, 'n_estimators': 1}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 3}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 10}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 30}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 100}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 300}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 1000}\n",
      "0.962 (+/-0.059) for {'max_depth': 3, 'n_estimators': 1}\n",
      "0.965 (+/-0.048) for {'max_depth': 3, 'n_estimators': 3}\n",
      "0.922 (+/-0.038) for {'max_depth': 3, 'n_estimators': 10}\n",
      "0.906 (+/-0.002) for {'max_depth': 3, 'n_estimators': 30}\n",
      "0.906 (+/-0.002) for {'max_depth': 3, 'n_estimators': 100}\n",
      "0.906 (+/-0.002) for {'max_depth': 3, 'n_estimators': 300}\n",
      "0.906 (+/-0.002) for {'max_depth': 3, 'n_estimators': 1000}\n",
      "0.969 (+/-0.017) for {'max_depth': 10, 'n_estimators': 1}\n",
      "0.972 (+/-0.012) for {'max_depth': 10, 'n_estimators': 3}\n",
      "0.972 (+/-0.009) for {'max_depth': 10, 'n_estimators': 10}\n",
      "0.975 (+/-0.007) for {'max_depth': 10, 'n_estimators': 30}\n",
      "0.975 (+/-0.012) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.975 (+/-0.01) for {'max_depth': 10, 'n_estimators': 300}\n",
      "0.976 (+/-0.012) for {'max_depth': 10, 'n_estimators': 1000}\n",
      "0.956 (+/-0.02) for {'max_depth': None, 'n_estimators': 1}\n",
      "0.966 (+/-0.008) for {'max_depth': None, 'n_estimators': 3}\n",
      "0.972 (+/-0.008) for {'max_depth': None, 'n_estimators': 10}\n",
      "0.972 (+/-0.008) for {'max_depth': None, 'n_estimators': 30}\n",
      "0.973 (+/-0.012) for {'max_depth': None, 'n_estimators': 100}\n",
      "0.973 (+/-0.01) for {'max_depth': None, 'n_estimators': 300}\n",
      "0.973 (+/-0.011) for {'max_depth': None, 'n_estimators': 1000}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.976\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'max_depth': 10, 'n_estimators': 1000}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86        55\n",
      "           1       0.98      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.93      0.92      0.92       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 47   8]\n",
      " [  7 455]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"n_estimators\": [1, 3, 10, 30, 100, 300, 1000], \"max_depth\": [1, 3, 10, None]}]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = GridSearchCV(RandomForestClassifier(random_state=0), param_grid, cv=cv)\n",
    "rfc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, rfc)\n",
    "summary[\"Random Forest\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914bcde-afdb-4ea0-93a5-4cc2fe2f9d39",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da015f64-7132-4a20-938c-4de52f430753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "     --------------------------------------- 99.8/99.8 MB 14.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\12039\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\12039\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from xgboost) (1.9.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12a311c8-7eec-4795-9a42-9c30d3355ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.09974146, 0.10755525, 0.09180932, 0.09690866, 0.10032735,\n",
      "       0.09130292, 0.07766566, 0.06495523, 0.06257343]), 'std_fit_time': array([0.01252798, 0.01582916, 0.00227681, 0.01334238, 0.01154239,\n",
      "       0.00320648, 0.00792783, 0.0014251 , 0.00102909]), 'mean_score_time': array([0.01296697, 0.01222668, 0.01271505, 0.01221008, 0.01279783,\n",
      "       0.01216421, 0.01244779, 0.01174259, 0.01165152]), 'std_score_time': array([7.26246076e-04, 1.16447385e-03, 4.22682546e-04, 1.24689098e-03,\n",
      "       4.16690474e-04, 5.12546936e-05, 4.44880288e-04, 1.15341167e-03,\n",
      "       1.57848589e-03]), 'param_reg_alpha': masked_array(data=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'reg_alpha': 0.001}, {'reg_alpha': 0.003}, {'reg_alpha': 0.01}, {'reg_alpha': 0.03}, {'reg_alpha': 0.1}, {'reg_alpha': 0.3}, {'reg_alpha': 1}, {'reg_alpha': 3}, {'reg_alpha': 10}], 'split0_test_score': array([0.97101449, 0.97342995, 0.97101449, 0.97342995, 0.97342995,\n",
      "       0.97584541, 0.97826087, 0.98067633, 0.98309179]), 'split1_test_score': array([0.96859903, 0.96859903, 0.96618357, 0.97101449, 0.97101449,\n",
      "       0.97342995, 0.97826087, 0.97826087, 0.97826087]), 'split2_test_score': array([0.98305085, 0.97578692, 0.98547215, 0.98305085, 0.98547215,\n",
      "       0.98305085, 0.98547215, 0.98547215, 0.98547215]), 'split3_test_score': array([0.95883777, 0.95883777, 0.95883777, 0.95883777, 0.95883777,\n",
      "       0.96368039, 0.968523  , 0.96610169, 0.96610169]), 'split4_test_score': array([0.968523  , 0.97094431, 0.968523  , 0.97336562, 0.97094431,\n",
      "       0.97336562, 0.97336562, 0.97336562, 0.97578692]), 'mean_test_score': array([0.97000503, 0.9695196 , 0.9700062 , 0.97193974, 0.97193974,\n",
      "       0.97387444, 0.9767765 , 0.97677533, 0.97774269]), 'std_test_score': array([0.00774717, 0.00585745, 0.00873795, 0.00774891, 0.00847178,\n",
      "       0.00620429, 0.00565288, 0.00661229, 0.00675224]), 'rank_test_score': array([8, 9, 7, 5, 6, 4, 2, 3, 1])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.97 (+/-0.015) for {'reg_alpha': 0.001}\n",
      "0.97 (+/-0.012) for {'reg_alpha': 0.003}\n",
      "0.97 (+/-0.017) for {'reg_alpha': 0.01}\n",
      "0.972 (+/-0.015) for {'reg_alpha': 0.03}\n",
      "0.972 (+/-0.017) for {'reg_alpha': 0.1}\n",
      "0.974 (+/-0.012) for {'reg_alpha': 0.3}\n",
      "0.977 (+/-0.011) for {'reg_alpha': 1}\n",
      "0.977 (+/-0.013) for {'reg_alpha': 3}\n",
      "0.978 (+/-0.014) for {'reg_alpha': 10}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.978\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'reg_alpha': 10}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        55\n",
      "           1       0.99      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.94      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 49   6]\n",
      " [  8 454]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"reg_alpha\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]}]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbc = GridSearchCV(XGBClassifier(random_state=0), param_grid, cv=cv)\n",
    "xgbc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, xgbc)\n",
    "summary[\"XGBoost\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b103c-d597-4344-b7f2-76d6f8d36919",
   "metadata": {},
   "source": [
    "### Linear SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5ada107-7827-44f2-b05c-65a887a59815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.01427002, 0.01955972, 0.01724691, 0.02276049, 0.02452269,\n",
      "       0.0246511 , 0.02480698]), 'std_fit_time': array([0.00664075, 0.00437733, 0.0014037 , 0.00576183, 0.00230959,\n",
      "       0.00155877, 0.00205269]), 'mean_score_time': array([0.00173674, 0.00129595, 0.0016057 , 0.00156651, 0.00152817,\n",
      "       0.00179057, 0.00174813]), 'std_score_time': array([0.00043011, 0.00043136, 0.00050365, 0.00042707, 0.00046257,\n",
      "       0.0004882 , 0.0003647 ]), 'param_C': masked_array(data=[0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
      "             mask=[False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01}, {'C': 0.03}, {'C': 0.1}, {'C': 0.3}, {'C': 1}, {'C': 3}, {'C': 10}], 'split0_test_score': array([0.94927536, 0.96135266, 0.97342995, 0.97826087, 0.9589372 ,\n",
      "       0.97584541, 0.97101449]), 'split1_test_score': array([0.94927536, 0.96618357, 0.97584541, 0.97584541, 0.97342995,\n",
      "       0.96859903, 0.96618357]), 'split2_test_score': array([0.96125908, 0.968523  , 0.97578692, 0.98062954, 0.91283293,\n",
      "       0.97578692, 0.97094431]), 'split3_test_score': array([0.93220339, 0.95157385, 0.95641646, 0.95883777, 0.95641646,\n",
      "       0.95883777, 0.95883777]), 'split4_test_score': array([0.94673123, 0.96610169, 0.97094431, 0.97336562, 0.968523  ,\n",
      "       0.88861985, 0.968523  ]), 'mean_test_score': array([0.94774889, 0.96274696, 0.97048461, 0.97338784, 0.95402791,\n",
      "       0.9535378 , 0.96710063]), 'std_test_score': array([0.00927236, 0.00605326, 0.00726157, 0.00766725, 0.0215099 ,\n",
      "       0.03305124, 0.00449958]), 'rank_test_score': array([7, 4, 2, 1, 5, 6, 3])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.948 (+/-0.019) for {'C': 0.01}\n",
      "0.963 (+/-0.012) for {'C': 0.03}\n",
      "0.97 (+/-0.015) for {'C': 0.1}\n",
      "0.973 (+/-0.015) for {'C': 0.3}\n",
      "0.954 (+/-0.043) for {'C': 1}\n",
      "0.954 (+/-0.066) for {'C': 3}\n",
      "0.967 (+/-0.009) for {'C': 10}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.973\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'C': 0.3}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85        55\n",
      "           1       0.98      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.91      0.92       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 46   9]\n",
      " [  7 455]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973,\n",
       " 'Linear SVMs': 0.969}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"C\": [0.01, 0.03, 0.1, 0.3, 1, 3, 10]}]\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = GridSearchCV(LinearSVC(random_state=0), param_grid, cv=cv)\n",
    "lsvc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, lsvc)\n",
    "summary[\"Linear SVMs\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a821b11-593d-4976-af23-0be8d49ff1b6",
   "metadata": {},
   "source": [
    "### Kernelized SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd4055ca-5169-48b7-bbea-66e42899598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.01743441, 0.01848555, 0.01735725, 0.02168794, 0.02325506,\n",
      "       0.03765216, 0.06712594, 0.07670684, 0.08873959, 0.02312164,\n",
      "       0.02181597, 0.01958241, 0.02189617, 0.03168807, 0.05600424,\n",
      "       0.07693968, 0.09323025, 0.09946046, 0.02374101, 0.02820382,\n",
      "       0.01891904, 0.02474079, 0.04011712, 0.06678138, 0.08585529,\n",
      "       0.08950229, 0.10329452, 0.01478748, 0.0151711 , 0.01209607,\n",
      "       0.01776061, 0.03058472, 0.05708709, 0.07530527, 0.0859818 ,\n",
      "       0.10610332, 0.01025319, 0.01348009, 0.01061773, 0.01630993,\n",
      "       0.02768607, 0.06402831, 0.0760066 , 0.08844728, 0.10542536,\n",
      "       0.00862675, 0.01407671, 0.01012387, 0.01579471, 0.02463427,\n",
      "       0.05506945, 0.07391648, 0.08505993, 0.10523367, 0.00860057,\n",
      "       0.01476412, 0.01396852, 0.01680245, 0.02275476, 0.05316606,\n",
      "       0.08260403, 0.08603396, 0.11348057]), 'std_fit_time': array([0.00306212, 0.00069709, 0.00085565, 0.00397294, 0.0009808 ,\n",
      "       0.0014921 , 0.00514174, 0.00113495, 0.00163614, 0.00641149,\n",
      "       0.00164939, 0.00061296, 0.00111938, 0.00216297, 0.00639211,\n",
      "       0.01281876, 0.0117541 , 0.00391761, 0.01071859, 0.00461133,\n",
      "       0.00191869, 0.00220537, 0.00168258, 0.00720879, 0.00667078,\n",
      "       0.00430862, 0.00263613, 0.00544716, 0.00068693, 0.0012845 ,\n",
      "       0.00067306, 0.00070274, 0.00503233, 0.00080837, 0.00156759,\n",
      "       0.0042409 , 0.00111741, 0.00189188, 0.00085169, 0.00102018,\n",
      "       0.00084344, 0.00217917, 0.00577862, 0.00725583, 0.00378372,\n",
      "       0.00053756, 0.00204548, 0.00089534, 0.00147255, 0.00275571,\n",
      "       0.00761474, 0.00240749, 0.00071743, 0.00393133, 0.000473  ,\n",
      "       0.00145611, 0.00157989, 0.00130224, 0.00162719, 0.00677238,\n",
      "       0.00373861, 0.00553836, 0.01306382]), 'mean_score_time': array([0.00946908, 0.00962763, 0.01025186, 0.01038151, 0.01309891,\n",
      "       0.02031112, 0.03264871, 0.03705544, 0.03854871, 0.01143885,\n",
      "       0.01166368, 0.01097512, 0.01188102, 0.01706595, 0.02985015,\n",
      "       0.03452029, 0.0405632 , 0.04153452, 0.01328545, 0.01199694,\n",
      "       0.00940576, 0.01288533, 0.0191761 , 0.03293366, 0.04737473,\n",
      "       0.03868656, 0.04168358, 0.0078949 , 0.00675478, 0.00588984,\n",
      "       0.00777888, 0.01346927, 0.026162  , 0.03489127, 0.03568511,\n",
      "       0.04398851, 0.00464983, 0.00599623, 0.00454602, 0.00673532,\n",
      "       0.0117579 , 0.02375159, 0.03173585, 0.04039836, 0.04213991,\n",
      "       0.00489569, 0.00572362, 0.00462275, 0.00649495, 0.01021724,\n",
      "       0.02209768, 0.03453803, 0.03664508, 0.04022689, 0.00388641,\n",
      "       0.00505424, 0.00515785, 0.00672379, 0.00997186, 0.02237782,\n",
      "       0.03625374, 0.03718915, 0.04278245]), 'std_score_time': array([0.00074326, 0.0004205 , 0.0007912 , 0.00106932, 0.00083983,\n",
      "       0.00104872, 0.0004631 , 0.00377069, 0.00163261, 0.00087509,\n",
      "       0.00050033, 0.00150919, 0.00087149, 0.00117532, 0.00361226,\n",
      "       0.00341996, 0.00296714, 0.00130088, 0.00699957, 0.00212241,\n",
      "       0.00072543, 0.00194061, 0.00111425, 0.00294224, 0.01858634,\n",
      "       0.00173925, 0.00130774, 0.00066424, 0.0003685 , 0.0006924 ,\n",
      "       0.00041306, 0.0009056 , 0.00092091, 0.00156966, 0.00098008,\n",
      "       0.00298971, 0.0005952 , 0.00058169, 0.00040491, 0.00045701,\n",
      "       0.00030774, 0.0014347 , 0.00434632, 0.00599212, 0.00277238,\n",
      "       0.00134023, 0.00089866, 0.00053083, 0.00099224, 0.00054019,\n",
      "       0.00072104, 0.00203357, 0.00088386, 0.00278193, 0.00039735,\n",
      "       0.00010488, 0.00050527, 0.00080626, 0.00041824, 0.00169856,\n",
      "       0.0023737 , 0.00224392, 0.00296668]), 'param_C': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=['scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01, 'gamma': 'scale'}, {'C': 0.01, 'gamma': 'auto'}, {'C': 0.01, 'gamma': 0.01}, {'C': 0.01, 'gamma': 0.03}, {'C': 0.01, 'gamma': 0.1}, {'C': 0.01, 'gamma': 0.3}, {'C': 0.01, 'gamma': 1}, {'C': 0.01, 'gamma': 3}, {'C': 0.01, 'gamma': 10}, {'C': 0.03, 'gamma': 'scale'}, {'C': 0.03, 'gamma': 'auto'}, {'C': 0.03, 'gamma': 0.01}, {'C': 0.03, 'gamma': 0.03}, {'C': 0.03, 'gamma': 0.1}, {'C': 0.03, 'gamma': 0.3}, {'C': 0.03, 'gamma': 1}, {'C': 0.03, 'gamma': 3}, {'C': 0.03, 'gamma': 10}, {'C': 0.1, 'gamma': 'scale'}, {'C': 0.1, 'gamma': 'auto'}, {'C': 0.1, 'gamma': 0.01}, {'C': 0.1, 'gamma': 0.03}, {'C': 0.1, 'gamma': 0.1}, {'C': 0.1, 'gamma': 0.3}, {'C': 0.1, 'gamma': 1}, {'C': 0.1, 'gamma': 3}, {'C': 0.1, 'gamma': 10}, {'C': 0.3, 'gamma': 'scale'}, {'C': 0.3, 'gamma': 'auto'}, {'C': 0.3, 'gamma': 0.01}, {'C': 0.3, 'gamma': 0.03}, {'C': 0.3, 'gamma': 0.1}, {'C': 0.3, 'gamma': 0.3}, {'C': 0.3, 'gamma': 1}, {'C': 0.3, 'gamma': 3}, {'C': 0.3, 'gamma': 10}, {'C': 1, 'gamma': 'scale'}, {'C': 1, 'gamma': 'auto'}, {'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.03}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 0.3}, {'C': 1, 'gamma': 1}, {'C': 1, 'gamma': 3}, {'C': 1, 'gamma': 10}, {'C': 3, 'gamma': 'scale'}, {'C': 3, 'gamma': 'auto'}, {'C': 3, 'gamma': 0.01}, {'C': 3, 'gamma': 0.03}, {'C': 3, 'gamma': 0.1}, {'C': 3, 'gamma': 0.3}, {'C': 3, 'gamma': 1}, {'C': 3, 'gamma': 3}, {'C': 3, 'gamma': 10}, {'C': 10, 'gamma': 'scale'}, {'C': 10, 'gamma': 'auto'}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.03}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 0.3}, {'C': 10, 'gamma': 1}, {'C': 10, 'gamma': 3}, {'C': 10, 'gamma': 10}], 'split0_test_score': array([0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.93961353,\n",
      "       0.98550725, 0.98550725, 0.98309179, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9589372 , 0.98550725,\n",
      "       0.98550725, 0.98550725, 0.97826087, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.97826087, 0.98550725, 0.98550725,\n",
      "       0.98309179, 0.98550725, 0.94927536, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.98550725, 0.98067633, 0.98550725, 0.98309179,\n",
      "       0.98309179, 0.97342995, 0.90821256, 0.90821256, 0.90821256,\n",
      "       0.98550725, 0.98309179, 0.98067633, 0.98309179, 0.97584541,\n",
      "       0.97101449, 0.90821256, 0.90821256, 0.90821256, 0.98309179,\n",
      "       0.98309179, 0.98309179, 0.97826087, 0.96618357, 0.96859903,\n",
      "       0.90821256, 0.90821256, 0.90821256]), 'split1_test_score': array([0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.94444444,\n",
      "       0.97826087, 0.97826087, 0.97826087, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.95169082, 0.97826087,\n",
      "       0.97826087, 0.97826087, 0.98067633, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.98067633, 0.97826087, 0.97826087,\n",
      "       0.97826087, 0.97826087, 0.95169082, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.97826087, 0.97826087, 0.97826087, 0.97826087,\n",
      "       0.97826087, 0.98067633, 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.97826087, 0.97826087, 0.97826087, 0.97826087, 0.97826087,\n",
      "       0.97584541, 0.90821256, 0.9057971 , 0.9057971 , 0.97826087,\n",
      "       0.97826087, 0.97826087, 0.97826087, 0.96859903, 0.97584541,\n",
      "       0.90821256, 0.9057971 , 0.9057971 ]), 'split2_test_score': array([0.90799031, 0.90799031, 0.90799031, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.90799031, 0.90799031, 0.90799031, 0.94915254,\n",
      "       0.98547215, 0.98547215, 0.98305085, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.90799031, 0.90799031, 0.96125908, 0.98547215,\n",
      "       0.98547215, 0.98547215, 0.98305085, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.90799031, 0.98547215, 0.98547215, 0.98547215,\n",
      "       0.98789346, 0.98789346, 0.95399516, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.98547215, 0.98789346, 0.98547215, 0.98789346,\n",
      "       0.98789346, 0.98062954, 0.90799031, 0.90799031, 0.90799031,\n",
      "       0.98547215, 0.98789346, 0.98789346, 0.98789346, 0.98547215,\n",
      "       0.97578692, 0.90799031, 0.90799031, 0.90799031, 0.98547215,\n",
      "       0.98789346, 0.98547215, 0.98789346, 0.98305085, 0.97578692,\n",
      "       0.90799031, 0.90799031, 0.90799031]), 'split3_test_score': array([0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.93220339,\n",
      "       0.968523  , 0.96610169, 0.968523  , 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.9346247 , 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.96610169, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.96368039, 0.968523  , 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.94915254, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.96610169, 0.968523  , 0.968523  , 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.90556901, 0.9031477 , 0.9031477 ,\n",
      "       0.968523  , 0.968523  , 0.968523  , 0.968523  , 0.968523  ,\n",
      "       0.96610169, 0.90556901, 0.9031477 , 0.9031477 , 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.968523  , 0.96610169, 0.96125908,\n",
      "       0.90556901, 0.9031477 , 0.9031477 ]), 'split4_test_score': array([0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.94188862,\n",
      "       0.96610169, 0.97578692, 0.96610169, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.94430993, 0.97578692,\n",
      "       0.97578692, 0.97578692, 0.96610169, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.97336562, 0.97578692, 0.97578692,\n",
      "       0.97578692, 0.97094431, 0.94673123, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.97578692, 0.97578692, 0.97578692, 0.97578692,\n",
      "       0.97578692, 0.968523  , 0.90799031, 0.90799031, 0.90799031,\n",
      "       0.97578692, 0.97578692, 0.97578692, 0.97578692, 0.97336562,\n",
      "       0.96125908, 0.91041162, 0.90799031, 0.90799031, 0.97578692,\n",
      "       0.97336562, 0.97578692, 0.97336562, 0.97336562, 0.96368039,\n",
      "       0.91041162, 0.90799031, 0.90799031]), 'mean_test_score': array([0.90614451, 0.90614451, 0.90614451, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.90614451, 0.90614451, 0.90614451, 0.9414605 ,\n",
      "       0.97677299, 0.97822578, 0.97580564, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.90614451, 0.90614451, 0.95016434, 0.97871004,\n",
      "       0.97871004, 0.97871004, 0.97483829, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.90614451, 0.97629107, 0.97871004, 0.97871004,\n",
      "       0.97871121, 0.97822578, 0.95016902, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.97822578, 0.97822812, 0.97871004, 0.97871121,\n",
      "       0.97871121, 0.97435636, 0.90711186, 0.9066276 , 0.9066276 ,\n",
      "       0.97871004, 0.97871121, 0.97822812, 0.97871121, 0.97629341,\n",
      "       0.97000152, 0.90807921, 0.9066276 , 0.9066276 , 0.97822695,\n",
      "       0.97822695, 0.97822695, 0.97726076, 0.97146015, 0.96903417,\n",
      "       0.90807921, 0.9066276 , 0.9066276 ]), 'std_test_score': array([0.00092852, 0.00092852, 0.00092852, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00092852, 0.00092852, 0.00092852, 0.00560999,\n",
      "       0.00819897, 0.00718989, 0.00719462, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00092852, 0.00092852, 0.00977897, 0.00639453,\n",
      "       0.00639453, 0.00639453, 0.00729245, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00092852, 0.00741607, 0.00639453, 0.00639453,\n",
      "       0.00657354, 0.00766353, 0.00247408, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00718989, 0.00631763, 0.00639453, 0.00657354,\n",
      "       0.00657354, 0.00544443, 0.00117166, 0.00195067, 0.00195067,\n",
      "       0.00639453, 0.00657354, 0.00631763, 0.00657354, 0.00560843,\n",
      "       0.00566193, 0.00153523, 0.00195067, 0.00195067, 0.0059377 ,\n",
      "       0.00685432, 0.0059377 , 0.00642768, 0.00636736, 0.00602158,\n",
      "       0.00153523, 0.00195067, 0.00195067]), 'rank_test_score': array([43, 43, 43, 43, 43, 43, 43, 43, 43, 33, 22, 18, 25, 43, 43, 43, 43,\n",
      "       43, 32,  6,  6,  6, 26, 43, 43, 43, 43, 24,  6,  6,  1, 18, 31, 43,\n",
      "       43, 43, 18, 13,  6,  1,  1, 27, 36, 37, 37,  6,  1, 13,  1, 23, 29,\n",
      "       34, 37, 37, 15, 15, 15, 21, 28, 30, 34, 37, 37])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 'scale'}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 'auto'}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 0.01}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 0.03}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 0.1}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 0.3}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 1}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 3}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 10}\n",
      "0.941 (+/-0.011) for {'C': 0.03, 'gamma': 'scale'}\n",
      "0.977 (+/-0.016) for {'C': 0.03, 'gamma': 'auto'}\n",
      "0.978 (+/-0.014) for {'C': 0.03, 'gamma': 0.01}\n",
      "0.976 (+/-0.014) for {'C': 0.03, 'gamma': 0.03}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 0.1}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 0.3}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 1}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 3}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 10}\n",
      "0.95 (+/-0.02) for {'C': 0.1, 'gamma': 'scale'}\n",
      "0.979 (+/-0.013) for {'C': 0.1, 'gamma': 'auto'}\n",
      "0.979 (+/-0.013) for {'C': 0.1, 'gamma': 0.01}\n",
      "0.979 (+/-0.013) for {'C': 0.1, 'gamma': 0.03}\n",
      "0.975 (+/-0.015) for {'C': 0.1, 'gamma': 0.1}\n",
      "0.906 (+/-0.002) for {'C': 0.1, 'gamma': 0.3}\n",
      "0.906 (+/-0.002) for {'C': 0.1, 'gamma': 1}\n",
      "0.906 (+/-0.002) for {'C': 0.1, 'gamma': 3}\n",
      "0.906 (+/-0.002) for {'C': 0.1, 'gamma': 10}\n",
      "0.976 (+/-0.015) for {'C': 0.3, 'gamma': 'scale'}\n",
      "0.979 (+/-0.013) for {'C': 0.3, 'gamma': 'auto'}\n",
      "0.979 (+/-0.013) for {'C': 0.3, 'gamma': 0.01}\n",
      "0.979 (+/-0.013) for {'C': 0.3, 'gamma': 0.03}\n",
      "0.978 (+/-0.015) for {'C': 0.3, 'gamma': 0.1}\n",
      "0.95 (+/-0.005) for {'C': 0.3, 'gamma': 0.3}\n",
      "0.906 (+/-0.002) for {'C': 0.3, 'gamma': 1}\n",
      "0.906 (+/-0.002) for {'C': 0.3, 'gamma': 3}\n",
      "0.906 (+/-0.002) for {'C': 0.3, 'gamma': 10}\n",
      "0.978 (+/-0.014) for {'C': 1, 'gamma': 'scale'}\n",
      "0.978 (+/-0.013) for {'C': 1, 'gamma': 'auto'}\n",
      "0.979 (+/-0.013) for {'C': 1, 'gamma': 0.01}\n",
      "0.979 (+/-0.013) for {'C': 1, 'gamma': 0.03}\n",
      "0.979 (+/-0.013) for {'C': 1, 'gamma': 0.1}\n",
      "0.974 (+/-0.011) for {'C': 1, 'gamma': 0.3}\n",
      "0.907 (+/-0.002) for {'C': 1, 'gamma': 1}\n",
      "0.907 (+/-0.004) for {'C': 1, 'gamma': 3}\n",
      "0.907 (+/-0.004) for {'C': 1, 'gamma': 10}\n",
      "0.979 (+/-0.013) for {'C': 3, 'gamma': 'scale'}\n",
      "0.979 (+/-0.013) for {'C': 3, 'gamma': 'auto'}\n",
      "0.978 (+/-0.013) for {'C': 3, 'gamma': 0.01}\n",
      "0.979 (+/-0.013) for {'C': 3, 'gamma': 0.03}\n",
      "0.976 (+/-0.011) for {'C': 3, 'gamma': 0.1}\n",
      "0.97 (+/-0.011) for {'C': 3, 'gamma': 0.3}\n",
      "0.908 (+/-0.003) for {'C': 3, 'gamma': 1}\n",
      "0.907 (+/-0.004) for {'C': 3, 'gamma': 3}\n",
      "0.907 (+/-0.004) for {'C': 3, 'gamma': 10}\n",
      "0.978 (+/-0.012) for {'C': 10, 'gamma': 'scale'}\n",
      "0.978 (+/-0.014) for {'C': 10, 'gamma': 'auto'}\n",
      "0.978 (+/-0.012) for {'C': 10, 'gamma': 0.01}\n",
      "0.977 (+/-0.013) for {'C': 10, 'gamma': 0.03}\n",
      "0.971 (+/-0.013) for {'C': 10, 'gamma': 0.1}\n",
      "0.969 (+/-0.012) for {'C': 10, 'gamma': 0.3}\n",
      "0.908 (+/-0.003) for {'C': 10, 'gamma': 1}\n",
      "0.907 (+/-0.004) for {'C': 10, 'gamma': 3}\n",
      "0.907 (+/-0.004) for {'C': 10, 'gamma': 10}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.979\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'C': 0.3, 'gamma': 0.03}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        55\n",
      "           1       0.99      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.94      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 49   6]\n",
      " [  8 454]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973,\n",
       " 'Linear SVMs': 0.969,\n",
       " 'Kernelized SVMs': 0.973}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"C\": [0.01, 0.03, 0.1, 0.3, 1, 3, 10], \"gamma\": [\"scale\", \"auto\", 0.01, 0.03, 0.1, 0.3, 1, 3, 10]}]\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = GridSearchCV(SVC(random_state=0), param_grid, cv=cv)\n",
    "svc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, svc)\n",
    "summary[\"Kernelized SVMs\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d26db3-bb28-4410-87f2-9bd2a590e399",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb1617a3-db7f-4155-8df8-24123d0532d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.18129659, 0.50103211, 0.57148833, 0.26352835, 0.58625808,\n",
      "       0.54303122, 0.85335183, 1.09590454, 0.94281211]), 'std_fit_time': array([0.01596171, 0.01342196, 0.08961589, 0.00711293, 0.02772268,\n",
      "       0.05128306, 0.03081536, 0.17652465, 0.07955283]), 'mean_score_time': array([0.00169311, 0.00220051, 0.00204635, 0.0020021 , 0.00194721,\n",
      "       0.00186572, 0.00161142, 0.00224957, 0.00207753]), 'std_score_time': array([5.44896753e-04, 3.99404013e-04, 4.91035783e-05, 6.86811056e-06,\n",
      "       2.14353413e-04, 2.55535365e-04, 4.80233268e-04, 3.98739677e-04,\n",
      "       3.40701401e-05]), 'param_hidden_layer_sizes': masked_array(data=[(10,), (10,), (10,), (30,), (30,), (30,), (100,),\n",
      "                   (100,), (100,)],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_solver': masked_array(data=['lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
      "                   'lbfgs', 'sgd', 'adam'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (10,), 'solver': 'lbfgs'}, {'hidden_layer_sizes': (10,), 'solver': 'sgd'}, {'hidden_layer_sizes': (10,), 'solver': 'adam'}, {'hidden_layer_sizes': (30,), 'solver': 'lbfgs'}, {'hidden_layer_sizes': (30,), 'solver': 'sgd'}, {'hidden_layer_sizes': (30,), 'solver': 'adam'}, {'hidden_layer_sizes': (100,), 'solver': 'lbfgs'}, {'hidden_layer_sizes': (100,), 'solver': 'sgd'}, {'hidden_layer_sizes': (100,), 'solver': 'adam'}], 'split0_test_score': array([0.96859903, 0.93961353, 0.97584541, 0.96376812, 0.94202899,\n",
      "       0.96618357, 0.97584541, 0.94202899, 0.97101449]), 'split1_test_score': array([0.97342995, 0.94444444, 0.97101449, 0.97342995, 0.94444444,\n",
      "       0.97101449, 0.96859903, 0.94444444, 0.97342995]), 'split2_test_score': array([0.97820823, 0.95883777, 0.97578692, 0.97578692, 0.95641646,\n",
      "       0.97578692, 0.97820823, 0.94673123, 0.97820823]), 'split3_test_score': array([0.96368039, 0.9346247 , 0.95157385, 0.96610169, 0.93220339,\n",
      "       0.95641646, 0.968523  , 0.93220339, 0.95157385]), 'split4_test_score': array([0.97094431, 0.94188862, 0.968523  , 0.96368039, 0.94430993,\n",
      "       0.97578692, 0.968523  , 0.94188862, 0.96610169]), 'mean_test_score': array([0.97097238, 0.94388181, 0.96854874, 0.96855341, 0.94388064,\n",
      "       0.96903768, 0.97193974, 0.94145933, 0.96806564]), 'std_test_score': array([0.00484157, 0.00814718, 0.00894271, 0.00507466, 0.00771279,\n",
      "       0.00724505, 0.00422035, 0.00495752, 0.00912338]), 'rank_test_score': array([2, 7, 5, 4, 8, 3, 1, 9, 6])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.971 (+/-0.01) for {'hidden_layer_sizes': (10,), 'solver': 'lbfgs'}\n",
      "0.944 (+/-0.016) for {'hidden_layer_sizes': (10,), 'solver': 'sgd'}\n",
      "0.969 (+/-0.018) for {'hidden_layer_sizes': (10,), 'solver': 'adam'}\n",
      "0.969 (+/-0.01) for {'hidden_layer_sizes': (30,), 'solver': 'lbfgs'}\n",
      "0.944 (+/-0.015) for {'hidden_layer_sizes': (30,), 'solver': 'sgd'}\n",
      "0.969 (+/-0.014) for {'hidden_layer_sizes': (30,), 'solver': 'adam'}\n",
      "0.972 (+/-0.008) for {'hidden_layer_sizes': (100,), 'solver': 'lbfgs'}\n",
      "0.941 (+/-0.01) for {'hidden_layer_sizes': (100,), 'solver': 'sgd'}\n",
      "0.968 (+/-0.018) for {'hidden_layer_sizes': (100,), 'solver': 'adam'}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.972\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'hidden_layer_sizes': (100,), 'solver': 'lbfgs'}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        55\n",
      "           1       0.98      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.93      0.93      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 48   7]\n",
      " [  7 455]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973,\n",
       " 'Linear SVMs': 0.969,\n",
       " 'Kernelized SVMs': 0.973,\n",
       " 'Neural Networks': 0.973}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"hidden_layer_sizes\": [(10,), (30,), (100,)], \"solver\": [\"lbfgs\", \"sgd\", \"adam\"]}]\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = GridSearchCV(MLPClassifier(random_state=0), param_grid, cv=cv)\n",
    "mlpc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, mlpc)\n",
    "summary[\"Neural Networks\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c7782-820d-424d-baa5-ea39189655ec",
   "metadata": {},
   "source": [
    "## Compare and choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bcb22f4-04cf-4207-85dd-1d1fe6ec11b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973,\n",
       " 'Linear SVMs': 0.969,\n",
       " 'Kernelized SVMs': 0.973,\n",
       " 'Neural Networks': 0.973}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b2a9d-2eb9-482c-a412-a16bbe211bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
