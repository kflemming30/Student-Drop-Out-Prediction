{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c15eec1-c7f5-4ac7-812f-8f0b965cefba",
   "metadata": {},
   "source": [
    "# University Student Dropout Prediction Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bf7a1-6f31-4126-9b2e-912e44642068",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this project, we aim to develop a machine learning model to predict the likelihood of university students dropping out. The challenge of student dropouts is a critical issue in higher education, impacting both the students' future and the educational institutions' effectiveness. Through predictive modeling, we seek to understand the key factors influencing dropout rates and identify at-risk students early in their academic journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9431d26-b335-4556-996e-5fda6897afd5",
   "metadata": {},
   "source": [
    "### Project Objectives:\n",
    "\n",
    "1. **Data Collection:** Acquire comprehensive and relevant datasets from universities, encompassing various factors like student demographics, academic records, engagement levels, and more.\n",
    "2. **Data Preprocessing:** Clean and preprocess the data to ensure accuracy and reliability for our predictive analysis.\n",
    "3. **Exploratory Data Analysis (EDA):** Perform in-depth analysis to uncover trends and insights within the data, guiding our feature selection and modeling approach.\n",
    "4.  **Development:** Construct a predictive model utilizing mehtods such as Random Forest, XGBoost, Gradient Boosting, and Feed-forward Neural Networks, leveraging their combined strengths.\n",
    "5. **Model Evaluation and Tuning:** Utilize relevant performance metrics to evaluate and refine the model, aiming for enhanced predictive accuracy and robustness.\n",
    "6. **Interpretation and Reporting:** Interpret the results to provide meaningful insights and recommendations, focusing on strategies to improve student retention rates at the university level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d5057-3c68-40c7-8b6e-ff53a819aa86",
   "metadata": {},
   "source": [
    "## 1. Data Collection:\n",
    "\n",
    "**Sources Include:**\n",
    "University requested student drop out data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a65927-53dc-455c-ae28-bd8d71f511bb",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "166baa13-8190-4ff4-ab46-1c7a2a764f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e8f73ce-82e6-40a9-9c68-55e5876f3505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PIDM</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>SEX</th>\n",
       "      <th>Degree</th>\n",
       "      <th>Major 1</th>\n",
       "      <th>1st Year GPA</th>\n",
       "      <th>Dorm</th>\n",
       "      <th>1st Year Retention</th>\n",
       "      <th>College</th>\n",
       "      <th>Total Earned Hours</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Major 2</th>\n",
       "      <th>Advisor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>2.49</td>\n",
       "      <td>Campion Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>SEC</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>Biology</td>\n",
       "      <td>3.18</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>1</td>\n",
       "      <td>CAS</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Regis Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>CAS</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>DSB Undeclared</td>\n",
       "      <td>3.84</td>\n",
       "      <td>Gonzaga Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>DSB</td>\n",
       "      <td>45</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>202109F</td>\n",
       "      <td>M</td>\n",
       "      <td>BS</td>\n",
       "      <td>Management</td>\n",
       "      <td>2.69</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>1</td>\n",
       "      <td>DSB</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PIDM   Cohort SEX Degree                 Major 1  1st Year GPA  \\\n",
       "0     1  202109F   M     BS  Mechanical Engineering          2.49   \n",
       "1     2  202109F   M     BS                 Biology          3.18   \n",
       "2     3  202109F   M     BS               Chemistry          2.86   \n",
       "3     4  202109F   M     BS          DSB Undeclared          3.84   \n",
       "4     5  202109F   M     BS              Management          2.69   \n",
       "\n",
       "           Dorm  1st Year Retention College  Total Earned Hours     SAT  \\\n",
       "0  Campion Hall                   1     SEC                  36     NaN   \n",
       "1      Commuter                   1     CAS                  47     NaN   \n",
       "2    Regis Hall                   1     CAS                  46     NaN   \n",
       "3  Gonzaga Hall                   1     DSB                  45  1300.0   \n",
       "4      Commuter                   1     DSB                  42     NaN   \n",
       "\n",
       "  Major 2  Advisor  \n",
       "0     NaN      1.0  \n",
       "1     NaN      2.0  \n",
       "2     NaN      3.0  \n",
       "3     NaN      4.0  \n",
       "4     NaN      5.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/kflemming30/Student-Drop-Out-Prediction/main/OIR_Student%20Data%20Request.csv\"\n",
    "student_df = pd.read_csv(url)\n",
    "student_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b31c2eba-3e5b-4e86-93d3-f57ab33741f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2584, 13)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c61839-0c40-4d50-89f3-4b21000a08af",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8dccfac0-f837-42a0-a5fc-d8ab0be3711f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2335\n",
       "0     249\n",
       "Name: 1st Year Retention, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df['1st Year Retention'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb3f177a-33c5-4ae9-919d-69c80618d159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PIDM</th>\n",
       "      <th>1st Year GPA</th>\n",
       "      <th>1st Year Retention</th>\n",
       "      <th>Total Earned Hours</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Advisor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2584.000000</td>\n",
       "      <td>2576.000000</td>\n",
       "      <td>2584.000000</td>\n",
       "      <td>2584.000000</td>\n",
       "      <td>632.000000</td>\n",
       "      <td>2576.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1292.500000</td>\n",
       "      <td>3.360839</td>\n",
       "      <td>0.903638</td>\n",
       "      <td>45.852167</td>\n",
       "      <td>1306.977848</td>\n",
       "      <td>56.611413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>746.080871</td>\n",
       "      <td>0.549521</td>\n",
       "      <td>0.295144</td>\n",
       "      <td>10.109841</td>\n",
       "      <td>88.935666</td>\n",
       "      <td>47.163027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>980.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>646.750000</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1292.500000</td>\n",
       "      <td>3.490000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1310.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1938.250000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1370.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2584.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1550.000000</td>\n",
       "      <td>166.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              PIDM  1st Year GPA  1st Year Retention  Total Earned Hours  \\\n",
       "count  2584.000000   2576.000000         2584.000000         2584.000000   \n",
       "mean   1292.500000      3.360839            0.903638           45.852167   \n",
       "std     746.080871      0.549521            0.295144           10.109841   \n",
       "min       1.000000      0.000000            0.000000            0.000000   \n",
       "25%     646.750000      3.110000            1.000000           45.000000   \n",
       "50%    1292.500000      3.490000            1.000000           46.000000   \n",
       "75%    1938.250000      3.750000            1.000000           51.000000   \n",
       "max    2584.000000      4.000000            1.000000           81.000000   \n",
       "\n",
       "               SAT      Advisor  \n",
       "count   632.000000  2576.000000  \n",
       "mean   1306.977848    56.611413  \n",
       "std      88.935666    47.163027  \n",
       "min     980.000000     0.000000  \n",
       "25%    1240.000000    11.000000  \n",
       "50%    1310.000000    53.000000  \n",
       "75%    1370.000000    88.000000  \n",
       "max    1550.000000   166.000000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "288e4d07-a5d8-4803-803b-ee2096103ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2584 entries, 0 to 2583\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   PIDM                2584 non-null   int64  \n",
      " 1   Cohort              2584 non-null   object \n",
      " 2   SEX                 2584 non-null   object \n",
      " 3   Degree              2584 non-null   object \n",
      " 4   Major 1             2584 non-null   object \n",
      " 5   1st Year GPA        2576 non-null   float64\n",
      " 6   Dorm                2584 non-null   object \n",
      " 7   1st Year Retention  2584 non-null   int64  \n",
      " 8   College             2584 non-null   object \n",
      " 9   Total Earned Hours  2584 non-null   int64  \n",
      " 10  SAT                 632 non-null    float64\n",
      " 11  Major 2             6 non-null      object \n",
      " 12  Advisor             2576 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(7)\n",
      "memory usage: 262.6+ KB\n"
     ]
    }
   ],
   "source": [
    "student_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e63e681c-53a6-4464-bdfb-57677bca7401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "College\n",
       "CAS     0.888528\n",
       "DSB     0.904889\n",
       "EGAN    0.931398\n",
       "SEC     0.916667\n",
       "Name: 1st Year Retention, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df.groupby('College')['1st Year Retention'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3264913d-ae23-44c3-b2a9-d4bbc69ff76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count of Students Retention</th>\n",
       "      <th>Mean 1st Year Retention</th>\n",
       "      <th>Mean Total Earned Hours</th>\n",
       "      <th>Mean 1st Year GPA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dorm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1036 North Benson Road</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>3.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jogues Hall</th>\n",
       "      <td>620</td>\n",
       "      <td>0.920968</td>\n",
       "      <td>47.885484</td>\n",
       "      <td>3.443441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Commuter</th>\n",
       "      <td>83</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>45.674699</td>\n",
       "      <td>3.237531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gonzaga Hall</th>\n",
       "      <td>402</td>\n",
       "      <td>0.915423</td>\n",
       "      <td>45.101990</td>\n",
       "      <td>3.350896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Campion Hall</th>\n",
       "      <td>496</td>\n",
       "      <td>0.915323</td>\n",
       "      <td>47.274194</td>\n",
       "      <td>3.319452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regis Hall</th>\n",
       "      <td>585</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>44.627350</td>\n",
       "      <td>3.342607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loyola Hall</th>\n",
       "      <td>395</td>\n",
       "      <td>0.858228</td>\n",
       "      <td>43.567089</td>\n",
       "      <td>3.343772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claver Hall</th>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>3.510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Count of Students Retention  Mean 1st Year Retention  \\\n",
       "Dorm                                                                           \n",
       "1036 North Benson Road                            1                 1.000000   \n",
       "Jogues Hall                                     620                 0.920968   \n",
       "Commuter                                         83                 0.915663   \n",
       "Gonzaga Hall                                    402                 0.915423   \n",
       "Campion Hall                                    496                 0.915323   \n",
       "Regis Hall                                      585                 0.897436   \n",
       "Loyola Hall                                     395                 0.858228   \n",
       "Claver Hall                                       2                 0.500000   \n",
       "\n",
       "                        Mean Total Earned Hours  Mean 1st Year GPA  \n",
       "Dorm                                                                \n",
       "1036 North Benson Road                36.000000           3.690000  \n",
       "Jogues Hall                           47.885484           3.443441  \n",
       "Commuter                              45.674699           3.237531  \n",
       "Gonzaga Hall                          45.101990           3.350896  \n",
       "Campion Hall                          47.274194           3.319452  \n",
       "Regis Hall                            44.627350           3.342607  \n",
       "Loyola Hall                           43.567089           3.343772  \n",
       "Claver Hall                           35.500000           3.510000  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by Dorm and calculate mean retention, mean total earned hours, and mean 1st year GPA, along with counts\n",
    "retention_mean_count = student_df.groupby('Dorm')['1st Year Retention'].agg(['count', 'mean']).sort_values(by='mean', ascending=False)\n",
    "earned_hours_mean = student_df.groupby('Dorm')['Total Earned Hours'].mean().sort_values(ascending=False)\n",
    "gpa_mean = student_df.groupby('Dorm')['1st Year GPA'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Merge the three Series on Dorm\n",
    "result = pd.concat([retention_mean_count, earned_hours_mean, gpa_mean], axis=1)\n",
    "\n",
    "# Add clarity to column names\n",
    "result.columns = ['Count of Students Retention', 'Mean 1st Year Retention', 'Mean Total Earned Hours', 'Mean 1st Year GPA']\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5bb58bd7-f039-4df0-9e0a-f15cf37e5142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>College</th>\n",
       "      <th>Major 1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">CAS</th>\n",
       "      <th>American Studies</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modern Languages</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Religious Studies</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economics</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Physics</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SEC</th>\n",
       "      <th>Biomedical Engineering</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computer Science</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOE Undeclared</th>\n",
       "      <td>0.935484</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EGAN</th>\n",
       "      <th>Nursing</th>\n",
       "      <td>0.935028</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">CAS</th>\n",
       "      <th>Sports Media</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chemistry</th>\n",
       "      <td>0.931034</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSB</th>\n",
       "      <th>DSB Undeclared</th>\n",
       "      <td>0.923554</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">CAS</th>\n",
       "      <th>Sociology and Anthropology</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biology</th>\n",
       "      <td>0.921986</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DSB</th>\n",
       "      <th>Business Analytics</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finance</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accounting</th>\n",
       "      <td>0.912500</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">CAS</th>\n",
       "      <th>Communication</th>\n",
       "      <td>0.904762</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undeclared</th>\n",
       "      <td>0.904255</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEC</th>\n",
       "      <th>Mechanical Engineering</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAS</th>\n",
       "      <th>History</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EGAN</th>\n",
       "      <th>Social Work</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSB</th>\n",
       "      <th>Management</th>\n",
       "      <td>0.881579</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EGAN</th>\n",
       "      <th>Public Health</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAS</th>\n",
       "      <th>Psychology</th>\n",
       "      <td>0.869110</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSB</th>\n",
       "      <th>Marketing</th>\n",
       "      <td>0.863354</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAS</th>\n",
       "      <th>Visual &amp; Performing Arts</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSB</th>\n",
       "      <th>Economics</th>\n",
       "      <td>0.846154</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEC</th>\n",
       "      <th>Electrical and Computer Engineering</th>\n",
       "      <td>0.846154</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">CAS</th>\n",
       "      <th>Mathematics</th>\n",
       "      <td>0.846154</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Digital Journalism</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politics</th>\n",
       "      <td>0.825000</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">DSB</th>\n",
       "      <th>International Business</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Information Systems &amp; Ops Mgmt</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">CAS</th>\n",
       "      <th>International Studies</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Program on the Environment</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 mean  count\n",
       "College Major 1                                             \n",
       "CAS     American Studies                     1.000000      2\n",
       "        Modern Languages                     1.000000      1\n",
       "        Religious Studies                    1.000000      1\n",
       "        Economics                            1.000000      7\n",
       "        Physics                              1.000000      5\n",
       "SEC     Biomedical Engineering               0.941176     17\n",
       "        Computer Science                     0.937500     48\n",
       "        SOE Undeclared                       0.935484     31\n",
       "EGAN    Nursing                              0.935028    354\n",
       "CAS     Sports Media                         0.933333     15\n",
       "        Chemistry                            0.931034     29\n",
       "DSB     DSB Undeclared                       0.923554    484\n",
       "CAS     Sociology and Anthropology           0.923077     13\n",
       "        Biology                              0.921986    141\n",
       "DSB     Business Analytics                   0.920000     25\n",
       "        Finance                              0.916667    240\n",
       "        Accounting                           0.912500     80\n",
       "CAS     Communication                        0.904762     63\n",
       "        Undeclared                           0.904255    282\n",
       "SEC     Mechanical Engineering               0.893617     47\n",
       "CAS     History                              0.888889     27\n",
       "EGAN    Social Work                          0.888889      9\n",
       "DSB     Management                           0.881579     76\n",
       "EGAN    Public Health                        0.875000     16\n",
       "CAS     Psychology                           0.869110    191\n",
       "DSB     Marketing                            0.863354    161\n",
       "CAS     Visual & Performing Arts             0.857143     28\n",
       "DSB     Economics                            0.846154     26\n",
       "SEC     Electrical and Computer Engineering  0.846154     13\n",
       "CAS     Mathematics                          0.846154     13\n",
       "        English                              0.840000     25\n",
       "        Digital Journalism                   0.833333      6\n",
       "        Politics                             0.825000     40\n",
       "DSB     International Business               0.818182     22\n",
       "        Information Systems & Ops Mgmt       0.818182     11\n",
       "CAS     International Studies                0.769231     13\n",
       "        Program on the Environment           0.727273     22"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df.groupby(['College', 'Major 1'])['1st Year Retention'].agg(['mean', 'count']).sort_values(by='mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3dfb9e7f-d05d-424d-8c16-05bec490c10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by College and Major 1, calculate mean retention and count, then sort\n",
    "grouped_data = student_df.groupby(['College', 'Major 1'])['1st Year Retention'].agg(['mean', 'count']).sort_values(by='mean', ascending=False).reset_index()\n",
    "\n",
    "# Plotting the scatter plot with switched axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=grouped_data, x='mean', y='count', hue='College', palette='viridis')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Mean Retention Rate vs Count by College and Major 1')\n",
    "plt.xlabel('Mean Retention Rate')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show plot\n",
    "plt.legend(title='College')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43721622-59f5-430f-a31f-5c6b65fd9c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bf960f9-368c-47f8-9eb7-a117b1c2860d",
   "metadata": {},
   "source": [
    "## 4. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47465b6d-f8db-420e-bec6-c82027682683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PIDM</th>\n",
       "      <th>1st Year GPA</th>\n",
       "      <th>1st Year Retention</th>\n",
       "      <th>Total Earned Hours</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Major 2</th>\n",
       "      <th>Advisor</th>\n",
       "      <th>Cohort_202109F</th>\n",
       "      <th>Cohort_202209F</th>\n",
       "      <th>SEX_F</th>\n",
       "      <th>...</th>\n",
       "      <th>Dorm_Claver Hall</th>\n",
       "      <th>Dorm_Commuter</th>\n",
       "      <th>Dorm_Gonzaga Hall</th>\n",
       "      <th>Dorm_Jogues Hall</th>\n",
       "      <th>Dorm_Loyola Hall</th>\n",
       "      <th>Dorm_Regis Hall</th>\n",
       "      <th>College_CAS</th>\n",
       "      <th>College_DSB</th>\n",
       "      <th>College_EGAN</th>\n",
       "      <th>College_SEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.18</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PIDM  1st Year GPA  1st Year Retention  Total Earned Hours     SAT Major 2  \\\n",
       "0     1          2.49                   1                  36     NaN     NaN   \n",
       "1     2          3.18                   1                  47     NaN     NaN   \n",
       "2     3          2.86                   1                  46     NaN     NaN   \n",
       "3     4          3.84                   1                  45  1300.0     NaN   \n",
       "4     5          2.69                   1                  42     NaN     NaN   \n",
       "\n",
       "   Advisor  Cohort_202109F  Cohort_202209F  SEX_F  ...  Dorm_Claver Hall  \\\n",
       "0      1.0               1               0      0  ...                 0   \n",
       "1      2.0               1               0      0  ...                 0   \n",
       "2      3.0               1               0      0  ...                 0   \n",
       "3      4.0               1               0      0  ...                 0   \n",
       "4      5.0               1               0      0  ...                 0   \n",
       "\n",
       "   Dorm_Commuter  Dorm_Gonzaga Hall  Dorm_Jogues Hall  Dorm_Loyola Hall  \\\n",
       "0              0                  0                 0                 0   \n",
       "1              1                  0                 0                 0   \n",
       "2              0                  0                 0                 0   \n",
       "3              0                  1                 0                 0   \n",
       "4              1                  0                 0                 0   \n",
       "\n",
       "   Dorm_Regis Hall  College_CAS  College_DSB  College_EGAN  College_SEC  \n",
       "0                0            0            0             0            1  \n",
       "1                0            1            0             0            0  \n",
       "2                1            1            0             0            0  \n",
       "3                0            0            1             0            0  \n",
       "4                0            0            1             0            0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Select categorical columns for one-hot encoding\n",
    "categorical_cols = ['Cohort', 'SEX', 'Degree', 'Major 1', 'Dorm', 'College']\n",
    "\n",
    "# Perform one-hot encoding\n",
    "student_df_encoded = pd.get_dummies(student_df, columns=categorical_cols)\n",
    "\n",
    "student_df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5aa785c-26a2-41eb-8902-c46e3c97c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2584 entries, 0 to 2583\n",
      "Data columns (total 62 columns):\n",
      " #   Column                                       Non-Null Count  Dtype  \n",
      "---  ------                                       --------------  -----  \n",
      " 0   PIDM                                         2584 non-null   int64  \n",
      " 1   1st Year GPA                                 2576 non-null   float64\n",
      " 2   1st Year Retention                           2584 non-null   int64  \n",
      " 3   Total Earned Hours                           2584 non-null   int64  \n",
      " 4   SAT                                          632 non-null    float64\n",
      " 5   Major 2                                      6 non-null      object \n",
      " 6   Advisor                                      2576 non-null   float64\n",
      " 7   Cohort_202109F                               2584 non-null   uint8  \n",
      " 8   Cohort_202209F                               2584 non-null   uint8  \n",
      " 9   SEX_F                                        2584 non-null   uint8  \n",
      " 10  SEX_M                                        2584 non-null   uint8  \n",
      " 11  Degree_BA                                    2584 non-null   uint8  \n",
      " 12  Degree_BS                                    2584 non-null   uint8  \n",
      " 13  Degree_BSW                                   2584 non-null   uint8  \n",
      " 14  Major 1_Accounting                           2584 non-null   uint8  \n",
      " 15  Major 1_American Studies                     2584 non-null   uint8  \n",
      " 16  Major 1_Biology                              2584 non-null   uint8  \n",
      " 17  Major 1_Biomedical Engineering               2584 non-null   uint8  \n",
      " 18  Major 1_Business Analytics                   2584 non-null   uint8  \n",
      " 19  Major 1_Chemistry                            2584 non-null   uint8  \n",
      " 20  Major 1_Communication                        2584 non-null   uint8  \n",
      " 21  Major 1_Computer Science                     2584 non-null   uint8  \n",
      " 22  Major 1_DSB Undeclared                       2584 non-null   uint8  \n",
      " 23  Major 1_Digital Journalism                   2584 non-null   uint8  \n",
      " 24  Major 1_Economics                            2584 non-null   uint8  \n",
      " 25  Major 1_Electrical and Computer Engineering  2584 non-null   uint8  \n",
      " 26  Major 1_English                              2584 non-null   uint8  \n",
      " 27  Major 1_Finance                              2584 non-null   uint8  \n",
      " 28  Major 1_History                              2584 non-null   uint8  \n",
      " 29  Major 1_Information Systems & Ops Mgmt       2584 non-null   uint8  \n",
      " 30  Major 1_International Business               2584 non-null   uint8  \n",
      " 31  Major 1_International Studies                2584 non-null   uint8  \n",
      " 32  Major 1_Management                           2584 non-null   uint8  \n",
      " 33  Major 1_Marketing                            2584 non-null   uint8  \n",
      " 34  Major 1_Mathematics                          2584 non-null   uint8  \n",
      " 35  Major 1_Mechanical Engineering               2584 non-null   uint8  \n",
      " 36  Major 1_Modern Languages                     2584 non-null   uint8  \n",
      " 37  Major 1_Nursing                              2584 non-null   uint8  \n",
      " 38  Major 1_Physics                              2584 non-null   uint8  \n",
      " 39  Major 1_Politics                             2584 non-null   uint8  \n",
      " 40  Major 1_Program on the Environment           2584 non-null   uint8  \n",
      " 41  Major 1_Psychology                           2584 non-null   uint8  \n",
      " 42  Major 1_Public Health                        2584 non-null   uint8  \n",
      " 43  Major 1_Religious Studies                    2584 non-null   uint8  \n",
      " 44  Major 1_SOE Undeclared                       2584 non-null   uint8  \n",
      " 45  Major 1_Social Work                          2584 non-null   uint8  \n",
      " 46  Major 1_Sociology and Anthropology           2584 non-null   uint8  \n",
      " 47  Major 1_Sports Media                         2584 non-null   uint8  \n",
      " 48  Major 1_Undeclared                           2584 non-null   uint8  \n",
      " 49  Major 1_Visual & Performing Arts             2584 non-null   uint8  \n",
      " 50  Dorm_1036 North Benson Road                  2584 non-null   uint8  \n",
      " 51  Dorm_Campion Hall                            2584 non-null   uint8  \n",
      " 52  Dorm_Claver Hall                             2584 non-null   uint8  \n",
      " 53  Dorm_Commuter                                2584 non-null   uint8  \n",
      " 54  Dorm_Gonzaga Hall                            2584 non-null   uint8  \n",
      " 55  Dorm_Jogues Hall                             2584 non-null   uint8  \n",
      " 56  Dorm_Loyola Hall                             2584 non-null   uint8  \n",
      " 57  Dorm_Regis Hall                              2584 non-null   uint8  \n",
      " 58  College_CAS                                  2584 non-null   uint8  \n",
      " 59  College_DSB                                  2584 non-null   uint8  \n",
      " 60  College_EGAN                                 2584 non-null   uint8  \n",
      " 61  College_SEC                                  2584 non-null   uint8  \n",
      "dtypes: float64(3), int64(3), object(1), uint8(55)\n",
      "memory usage: 280.2+ KB\n"
     ]
    }
   ],
   "source": [
    "student_df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed39ce8e-8b9b-4f38-a50f-d1d873bea069",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df_encoded = student_df_encoded.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07baa4c7-1c4b-44d5-8e1d-c843e4fdc6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2584 entries, 0 to 2583\n",
      "Data columns (total 58 columns):\n",
      " #   Column                                       Non-Null Count  Dtype\n",
      "---  ------                                       --------------  -----\n",
      " 0   PIDM                                         2584 non-null   int64\n",
      " 1   1st Year Retention                           2584 non-null   int64\n",
      " 2   Total Earned Hours                           2584 non-null   int64\n",
      " 3   Cohort_202109F                               2584 non-null   uint8\n",
      " 4   Cohort_202209F                               2584 non-null   uint8\n",
      " 5   SEX_F                                        2584 non-null   uint8\n",
      " 6   SEX_M                                        2584 non-null   uint8\n",
      " 7   Degree_BA                                    2584 non-null   uint8\n",
      " 8   Degree_BS                                    2584 non-null   uint8\n",
      " 9   Degree_BSW                                   2584 non-null   uint8\n",
      " 10  Major 1_Accounting                           2584 non-null   uint8\n",
      " 11  Major 1_American Studies                     2584 non-null   uint8\n",
      " 12  Major 1_Biology                              2584 non-null   uint8\n",
      " 13  Major 1_Biomedical Engineering               2584 non-null   uint8\n",
      " 14  Major 1_Business Analytics                   2584 non-null   uint8\n",
      " 15  Major 1_Chemistry                            2584 non-null   uint8\n",
      " 16  Major 1_Communication                        2584 non-null   uint8\n",
      " 17  Major 1_Computer Science                     2584 non-null   uint8\n",
      " 18  Major 1_DSB Undeclared                       2584 non-null   uint8\n",
      " 19  Major 1_Digital Journalism                   2584 non-null   uint8\n",
      " 20  Major 1_Economics                            2584 non-null   uint8\n",
      " 21  Major 1_Electrical and Computer Engineering  2584 non-null   uint8\n",
      " 22  Major 1_English                              2584 non-null   uint8\n",
      " 23  Major 1_Finance                              2584 non-null   uint8\n",
      " 24  Major 1_History                              2584 non-null   uint8\n",
      " 25  Major 1_Information Systems & Ops Mgmt       2584 non-null   uint8\n",
      " 26  Major 1_International Business               2584 non-null   uint8\n",
      " 27  Major 1_International Studies                2584 non-null   uint8\n",
      " 28  Major 1_Management                           2584 non-null   uint8\n",
      " 29  Major 1_Marketing                            2584 non-null   uint8\n",
      " 30  Major 1_Mathematics                          2584 non-null   uint8\n",
      " 31  Major 1_Mechanical Engineering               2584 non-null   uint8\n",
      " 32  Major 1_Modern Languages                     2584 non-null   uint8\n",
      " 33  Major 1_Nursing                              2584 non-null   uint8\n",
      " 34  Major 1_Physics                              2584 non-null   uint8\n",
      " 35  Major 1_Politics                             2584 non-null   uint8\n",
      " 36  Major 1_Program on the Environment           2584 non-null   uint8\n",
      " 37  Major 1_Psychology                           2584 non-null   uint8\n",
      " 38  Major 1_Public Health                        2584 non-null   uint8\n",
      " 39  Major 1_Religious Studies                    2584 non-null   uint8\n",
      " 40  Major 1_SOE Undeclared                       2584 non-null   uint8\n",
      " 41  Major 1_Social Work                          2584 non-null   uint8\n",
      " 42  Major 1_Sociology and Anthropology           2584 non-null   uint8\n",
      " 43  Major 1_Sports Media                         2584 non-null   uint8\n",
      " 44  Major 1_Undeclared                           2584 non-null   uint8\n",
      " 45  Major 1_Visual & Performing Arts             2584 non-null   uint8\n",
      " 46  Dorm_1036 North Benson Road                  2584 non-null   uint8\n",
      " 47  Dorm_Campion Hall                            2584 non-null   uint8\n",
      " 48  Dorm_Claver Hall                             2584 non-null   uint8\n",
      " 49  Dorm_Commuter                                2584 non-null   uint8\n",
      " 50  Dorm_Gonzaga Hall                            2584 non-null   uint8\n",
      " 51  Dorm_Jogues Hall                             2584 non-null   uint8\n",
      " 52  Dorm_Loyola Hall                             2584 non-null   uint8\n",
      " 53  Dorm_Regis Hall                              2584 non-null   uint8\n",
      " 54  College_CAS                                  2584 non-null   uint8\n",
      " 55  College_DSB                                  2584 non-null   uint8\n",
      " 56  College_EGAN                                 2584 non-null   uint8\n",
      " 57  College_SEC                                  2584 non-null   uint8\n",
      "dtypes: int64(3), uint8(55)\n",
      "memory usage: 199.5 KB\n"
     ]
    }
   ],
   "source": [
    "student_df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32055311-4f34-49ca-a443-b21dad274302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2335\n",
       "0     249\n",
       "Name: 1st Year Retention, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df_encoded['1st Year Retention'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92ba14d6-eb4e-45b0-98c1-085d8a541268",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = student_df_encoded.columns[(student_df_encoded.columns != 'PIDM') & (student_df_encoded.columns != '1st Year Retention')]\n",
    "target = \"1st Year Retention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "130cc9e8-4c3e-42b6-89f4-2a225195ad4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2067, 56), (2067,), (517, 56), (517,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = student_df_encoded[features]\n",
    "y = student_df_encoded[target]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4c56b8f-b28f-43e0-b39e-b64efb7ad1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_test2(X_train, X_test, y_train, y_test, param_grid, clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"*** Parameter estimation results: \")\n",
    "    print(clf.cv_results_)\n",
    "    print()\n",
    "\n",
    "    print(\"*** Grid scores: \")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "\n",
    "    for mean, std, param in zip(means, stds, params):\n",
    "        print(f\"{round(mean, 3)} (+/-{round(std*2, 3)}) for {param}\")\n",
    "    print()\n",
    "\n",
    "    print(\"*** Highest accuracy score: \")\n",
    "    print(f\"{round(clf.best_score_, 3)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"*** Best parameters set found: \")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "    print(\"*** Classification report for the best parameters set: \")\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    print(\"*** Confusion matrix for the best parameters set: \")\n",
    "    print(metrics.confusion_matrix(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    print(\"*** Final accuracy score: \")\n",
    "    test_score = round(clf.score(X_test, y_test), 3)\n",
    "    print(test_score)\n",
    "\n",
    "    return clf, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5498129d-903c-4492-93e5-86657c782da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "summary = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fee27-11a6-4513-87d8-49797f80460a",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15bc9fc9-94d7-4104-a35c-69318d534c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\"n_neighbors\": [1, 3, 10, 30, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec9c7930-9ff4-4f7c-af02-575a2f6fdad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid=[{&#x27;n_neighbors&#x27;: [1, 3, 10, 30, 100]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid=[{&#x27;n_neighbors&#x27;: [1, 3, 10, 30, 100]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid=[{'n_neighbors': [1, 3, 10, 30, 100]}])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knc = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cv)\n",
    "knc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c60d1d5f-3345-43a3-8658-f8d7d791f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.00243521, 0.00223074, 0.0020205 , 0.00160122, 0.00220819]), 'std_fit_time': array([7.87534685e-04, 3.45970599e-04, 1.99814684e-05, 4.92567730e-04,\n",
      "       4.16068890e-04]), 'mean_score_time': array([0.03127427, 0.01286249, 0.01755128, 0.01739321, 0.01890998]), 'std_score_time': array([0.03773044, 0.00069117, 0.00020119, 0.00036096, 0.00026017]), 'param_n_neighbors': masked_array(data=[1, 3, 10, 30, 100],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 10}, {'n_neighbors': 30}, {'n_neighbors': 100}], 'split0_test_score': array([0.96135266, 0.98309179, 0.98067633, 0.98550725, 0.98550725]), 'split1_test_score': array([0.96376812, 0.97342995, 0.97826087, 0.97826087, 0.97826087]), 'split2_test_score': array([0.97578692, 0.98547215, 0.98789346, 0.98547215, 0.98547215]), 'split3_test_score': array([0.95883777, 0.968523  , 0.968523  , 0.968523  , 0.968523  ]), 'split4_test_score': array([0.96610169, 0.97578692, 0.97578692, 0.97578692, 0.97578692]), 'mean_test_score': array([0.96516943, 0.97726076, 0.97822812, 0.97871004, 0.97871004]), 'std_test_score': array([0.00583475, 0.00623897, 0.00631763, 0.00639453, 0.00639453]), 'rank_test_score': array([5, 4, 3, 1, 1])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.965 (+/-0.012) for {'n_neighbors': 1}\n",
      "0.977 (+/-0.012) for {'n_neighbors': 3}\n",
      "0.978 (+/-0.013) for {'n_neighbors': 10}\n",
      "0.979 (+/-0.013) for {'n_neighbors': 30}\n",
      "0.979 (+/-0.013) for {'n_neighbors': 100}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.979\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'n_neighbors': 30}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        55\n",
      "           1       0.99      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.94      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 49   6]\n",
      " [  8 454]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    }
   ],
   "source": [
    "knc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, knc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f2cae2a-9544-46e3-84d9-902371b4c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[\"k-NNs\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4b04b-d9de-417a-ade5-188697432b81",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61408d38-cd9d-450f-83da-55ab914c16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.0215632 , 0.00390429, 0.01907907, 0.00313249, 0.03486347,\n",
      "       0.00342536, 0.04339461, 0.0044951 , 0.03844342, 0.00390282,\n",
      "       0.03828588, 0.00427518, 0.04030013, 0.00479379]), 'std_fit_time': array([9.22369786e-03, 1.56588313e-03, 2.52038365e-03, 4.72469834e-05,\n",
      "       2.04485439e-02, 4.67491962e-04, 7.58949490e-03, 4.69298940e-04,\n",
      "       5.64246610e-03, 4.57270565e-04, 4.42928458e-03, 5.54292926e-04,\n",
      "       3.00795538e-03, 3.86780152e-04]), 'mean_score_time': array([0.00105004, 0.00126553, 0.00123997, 0.00170135, 0.00180221,\n",
      "       0.0015029 , 0.00200558, 0.00183668, 0.00175686, 0.00140724,\n",
      "       0.00225692, 0.0012094 , 0.00175066, 0.00160408]), 'std_score_time': array([8.13531427e-05, 4.88019455e-04, 7.47397510e-04, 5.60226904e-04,\n",
      "       3.96371042e-04, 4.34585802e-04, 5.13126413e-06, 3.50985532e-04,\n",
      "       4.60850555e-04, 4.52566185e-04, 3.80552184e-04, 3.76175368e-04,\n",
      "       6.08562634e-04, 4.92353081e-04]), 'param_C': masked_array(data=[0.01, 0.01, 0.03, 0.03, 0.1, 0.1, 0.3, 0.3, 1, 1, 3, 3,\n",
      "                   10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_solver': masked_array(data=['lbfgs', 'liblinear', 'lbfgs', 'liblinear', 'lbfgs',\n",
      "                   'liblinear', 'lbfgs', 'liblinear', 'lbfgs',\n",
      "                   'liblinear', 'lbfgs', 'liblinear', 'lbfgs',\n",
      "                   'liblinear'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01, 'solver': 'lbfgs'}, {'C': 0.01, 'solver': 'liblinear'}, {'C': 0.03, 'solver': 'lbfgs'}, {'C': 0.03, 'solver': 'liblinear'}, {'C': 0.1, 'solver': 'lbfgs'}, {'C': 0.1, 'solver': 'liblinear'}, {'C': 0.3, 'solver': 'lbfgs'}, {'C': 0.3, 'solver': 'liblinear'}, {'C': 1, 'solver': 'lbfgs'}, {'C': 1, 'solver': 'liblinear'}, {'C': 3, 'solver': 'lbfgs'}, {'C': 3, 'solver': 'liblinear'}, {'C': 10, 'solver': 'lbfgs'}, {'C': 10, 'solver': 'liblinear'}], 'split0_test_score': array([0.97826087, 0.92270531, 0.97826087, 0.93961353, 0.98067633,\n",
      "       0.94202899, 0.98067633, 0.96135266, 0.97826087, 0.96618357,\n",
      "       0.96859903, 0.97101449, 0.96618357, 0.97101449]), 'split1_test_score': array([0.97584541, 0.92512077, 0.97826087, 0.94444444, 0.98067633,\n",
      "       0.9468599 , 0.97826087, 0.96376812, 0.97584541, 0.97101449,\n",
      "       0.97342995, 0.97342995, 0.97342995, 0.97342995]), 'split2_test_score': array([0.98305085, 0.93220339, 0.98305085, 0.94673123, 0.98305085,\n",
      "       0.95883777, 0.98062954, 0.97094431, 0.98305085, 0.97336562,\n",
      "       0.97820823, 0.97820823, 0.97820823, 0.97820823]), 'split3_test_score': array([0.96368039, 0.91283293, 0.96610169, 0.93220339, 0.96610169,\n",
      "       0.92736077, 0.968523  , 0.94673123, 0.96368039, 0.96125908,\n",
      "       0.96368039, 0.96368039, 0.96368039, 0.96368039]), 'split4_test_score': array([0.97336562, 0.9346247 , 0.97094431, 0.94188862, 0.97094431,\n",
      "       0.94430993, 0.97336562, 0.96610169, 0.97336562, 0.968523  ,\n",
      "       0.97336562, 0.97336562, 0.97336562, 0.97336562]), 'mean_test_score': array([0.97484063, 0.92549742, 0.97532372, 0.94097624, 0.9762899 ,\n",
      "       0.94387947, 0.97629107, 0.9617796 , 0.97484063, 0.96806915,\n",
      "       0.97145664, 0.97193974, 0.97097355, 0.97193974]), 'std_test_score': array([0.00643183, 0.007702  , 0.00601975, 0.00499601, 0.00658129,\n",
      "       0.0100907 , 0.00470816, 0.00816409, 0.00643183, 0.00416812,\n",
      "       0.00493474, 0.00474596, 0.00529586, 0.00474596]), 'rank_test_score': array([ 4, 14,  3, 13,  2, 12,  1, 11,  4, 10,  8,  6,  9,  6])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.975 (+/-0.013) for {'C': 0.01, 'solver': 'lbfgs'}\n",
      "0.925 (+/-0.015) for {'C': 0.01, 'solver': 'liblinear'}\n",
      "0.975 (+/-0.012) for {'C': 0.03, 'solver': 'lbfgs'}\n",
      "0.941 (+/-0.01) for {'C': 0.03, 'solver': 'liblinear'}\n",
      "0.976 (+/-0.013) for {'C': 0.1, 'solver': 'lbfgs'}\n",
      "0.944 (+/-0.02) for {'C': 0.1, 'solver': 'liblinear'}\n",
      "0.976 (+/-0.009) for {'C': 0.3, 'solver': 'lbfgs'}\n",
      "0.962 (+/-0.016) for {'C': 0.3, 'solver': 'liblinear'}\n",
      "0.975 (+/-0.013) for {'C': 1, 'solver': 'lbfgs'}\n",
      "0.968 (+/-0.008) for {'C': 1, 'solver': 'liblinear'}\n",
      "0.971 (+/-0.01) for {'C': 3, 'solver': 'lbfgs'}\n",
      "0.972 (+/-0.009) for {'C': 3, 'solver': 'liblinear'}\n",
      "0.971 (+/-0.011) for {'C': 10, 'solver': 'lbfgs'}\n",
      "0.972 (+/-0.009) for {'C': 10, 'solver': 'liblinear'}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.976\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'C': 0.3, 'solver': 'lbfgs'}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        55\n",
      "           1       0.98      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.93      0.93      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 48   7]\n",
      " [  7 455]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973, 'Logistic Regression': 0.973}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"C\": [0.01, 0.03, 0.1, 0.3, 1, 3, 10], \"solver\": [\"lbfgs\", \"liblinear\"]}]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "lr, score = train_test2(X_train, X_test, y_train, y_test, param_grid, lr)\n",
    "summary[\"Logistic Regression\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ebf5f-ec6c-4c46-88c7-9826f8cd041e",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3f37fed-68c3-4c6f-af09-845f6fec303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.0026176 , 0.0021596 , 0.00403996, 0.00420666]), 'std_fit_time': array([0.00078758, 0.00026445, 0.00061224, 0.00016043]), 'mean_score_time': array([0.00118637, 0.00084114, 0.00124307, 0.00124216]), 'std_score_time': array([0.00040843, 0.00031782, 0.00040774, 0.00043249]), 'param_max_depth': masked_array(data=[1, 3, 10, None],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 1}, {'max_depth': 3}, {'max_depth': 10}, {'max_depth': None}], 'split0_test_score': array([0.98309179, 0.98067633, 0.96859903, 0.96376812]), 'split1_test_score': array([0.97826087, 0.96618357, 0.96135266, 0.96135266]), 'split2_test_score': array([0.98547215, 0.98062954, 0.97578692, 0.96368039]), 'split3_test_score': array([0.96610169, 0.96610169, 0.95641646, 0.95399516]), 'split4_test_score': array([0.97578692, 0.968523  , 0.94430993, 0.94915254]), 'mean_test_score': array([0.97774269, 0.97242283, 0.961293  , 0.95838977]), 'std_test_score': array([0.00675224, 0.0067759 , 0.01072997, 0.00583669]), 'rank_test_score': array([1, 2, 3, 4])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.978 (+/-0.014) for {'max_depth': 1}\n",
      "0.972 (+/-0.014) for {'max_depth': 3}\n",
      "0.961 (+/-0.021) for {'max_depth': 10}\n",
      "0.958 (+/-0.012) for {'max_depth': None}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.978\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'max_depth': 1}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        55\n",
      "           1       0.99      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.94      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 49   6]\n",
      " [  8 454]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973, 'Logistic Regression': 0.973, 'Decision Trees': 0.973}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"max_depth\": [1, 3, 10, None]}]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=cv)\n",
    "dtc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, dtc)\n",
    "summary[\"Decision Trees\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1c1c2-d0d8-4a72-a2ee-eb4654d5d9f0",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bdfd200-b957-42a4-814c-f961754b1dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.00243082, 0.00404325, 0.01063366, 0.02723708, 0.08142266,\n",
      "       0.24292064, 0.89854703, 0.00383501, 0.00630159, 0.01318731,\n",
      "       0.035848  , 0.10382023, 0.26925354, 0.83909163, 0.00324631,\n",
      "       0.0052053 , 0.01457281, 0.03887706, 0.12820635, 0.38322115,\n",
      "       1.412216  , 0.00422063, 0.00789852, 0.0179563 , 0.05732584,\n",
      "       0.15936494, 0.44913168, 1.45396571]), 'std_fit_time': array([0.0004871 , 0.00038824, 0.00104991, 0.00193204, 0.00311806,\n",
      "       0.0111895 , 0.04850224, 0.00042067, 0.00076142, 0.00072632,\n",
      "       0.00601859, 0.0047691 , 0.02361128, 0.03612725, 0.00071353,\n",
      "       0.00012275, 0.00099274, 0.00266393, 0.01058599, 0.01520361,\n",
      "       0.12264897, 0.0009483 , 0.00105364, 0.00102578, 0.00491704,\n",
      "       0.01025284, 0.01686361, 0.05550145]), 'mean_score_time': array([0.0014513 , 0.00131979, 0.00171824, 0.00249515, 0.0050158 ,\n",
      "       0.0133388 , 0.04624476, 0.00188265, 0.00202985, 0.00242939,\n",
      "       0.00332813, 0.00684423, 0.01463027, 0.04217482, 0.00114608,\n",
      "       0.00162005, 0.00192709, 0.00335298, 0.0066854 , 0.01672788,\n",
      "       0.05983844, 0.0018621 , 0.00168223, 0.00259695, 0.00462303,\n",
      "       0.00830932, 0.01827884, 0.05935822]), 'std_score_time': array([5.05053785e-04, 4.86668460e-04, 5.01400437e-04, 5.30302384e-04,\n",
      "       4.44727523e-04, 1.15378187e-03, 5.50491397e-03, 2.04145376e-04,\n",
      "       3.07825624e-05, 4.81693373e-04, 4.50275447e-04, 1.08396275e-03,\n",
      "       2.50911576e-03, 3.41453135e-03, 4.78377144e-04, 5.10336804e-04,\n",
      "       3.40196208e-04, 4.25887598e-04, 5.56273017e-04, 1.04721629e-03,\n",
      "       5.15423044e-03, 8.51873479e-04, 8.09776192e-04, 4.90322879e-04,\n",
      "       9.91969907e-04, 1.59163638e-03, 1.12636098e-03, 4.88788223e-03]), 'param_max_depth': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 10, 10, 10,\n",
      "                   10, 10, 10, 10, None, None, None, None, None, None,\n",
      "                   None],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[1, 3, 10, 30, 100, 300, 1000, 1, 3, 10, 30, 100, 300,\n",
      "                   1000, 1, 3, 10, 30, 100, 300, 1000, 1, 3, 10, 30, 100,\n",
      "                   300, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 1, 'n_estimators': 1}, {'max_depth': 1, 'n_estimators': 3}, {'max_depth': 1, 'n_estimators': 10}, {'max_depth': 1, 'n_estimators': 30}, {'max_depth': 1, 'n_estimators': 100}, {'max_depth': 1, 'n_estimators': 300}, {'max_depth': 1, 'n_estimators': 1000}, {'max_depth': 3, 'n_estimators': 1}, {'max_depth': 3, 'n_estimators': 3}, {'max_depth': 3, 'n_estimators': 10}, {'max_depth': 3, 'n_estimators': 30}, {'max_depth': 3, 'n_estimators': 100}, {'max_depth': 3, 'n_estimators': 300}, {'max_depth': 3, 'n_estimators': 1000}, {'max_depth': 10, 'n_estimators': 1}, {'max_depth': 10, 'n_estimators': 3}, {'max_depth': 10, 'n_estimators': 10}, {'max_depth': 10, 'n_estimators': 30}, {'max_depth': 10, 'n_estimators': 100}, {'max_depth': 10, 'n_estimators': 300}, {'max_depth': 10, 'n_estimators': 1000}, {'max_depth': None, 'n_estimators': 1}, {'max_depth': None, 'n_estimators': 3}, {'max_depth': None, 'n_estimators': 10}, {'max_depth': None, 'n_estimators': 30}, {'max_depth': None, 'n_estimators': 100}, {'max_depth': None, 'n_estimators': 300}, {'max_depth': None, 'n_estimators': 1000}], 'split0_test_score': array([0.89613527, 0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.98550725, 0.98550725, 0.90821256,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.97826087,\n",
      "       0.96618357, 0.96859903, 0.97342995, 0.97826087, 0.98067633,\n",
      "       0.98309179, 0.96376812, 0.97101449, 0.97826087, 0.97584541,\n",
      "       0.97342995, 0.97584541, 0.97584541]), 'split1_test_score': array([0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.92028986, 0.91545894,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.97101449,\n",
      "       0.97826087, 0.97342995, 0.97826087, 0.98067633, 0.97826087,\n",
      "       0.98067633, 0.93719807, 0.96135266, 0.97342995, 0.97342995,\n",
      "       0.97342995, 0.97342995, 0.97584541]), 'split2_test_score': array([0.90799031, 0.90799031, 0.90799031, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.90799031, 0.98547215, 0.98547215, 0.91767554,\n",
      "       0.90799031, 0.90799031, 0.90799031, 0.90799031, 0.97578692,\n",
      "       0.97820823, 0.97820823, 0.97820823, 0.98062954, 0.97820823,\n",
      "       0.97820823, 0.96368039, 0.968523  , 0.97094431, 0.97578692,\n",
      "       0.98305085, 0.98062954, 0.98062954]), 'split3_test_score': array([0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.95883777, 0.96125908, 0.95883777,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.95641646,\n",
      "       0.96368039, 0.96610169, 0.968523  , 0.96610169, 0.96610169,\n",
      "       0.968523  , 0.95399516, 0.96125908, 0.96610169, 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.968523  ]), 'split4_test_score': array([0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.97336562, 0.97336562, 0.90799031,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.96125908,\n",
      "       0.97336562, 0.97578692, 0.97578692, 0.97094431, 0.97336562,\n",
      "       0.968523  , 0.96125908, 0.968523  , 0.97336562, 0.96610169,\n",
      "       0.96610169, 0.96610169, 0.96610169]), 'mean_test_score': array([0.90421214, 0.90614451, 0.90614451, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.90614451, 0.96179598, 0.96517879, 0.92163503,\n",
      "       0.90614451, 0.90614451, 0.90614451, 0.90614451, 0.96854757,\n",
      "       0.97193974, 0.97242517, 0.9748418 , 0.97532255, 0.97532255,\n",
      "       0.97580447, 0.95598016, 0.96613445, 0.97242049, 0.9719374 ,\n",
      "       0.97290709, 0.97290592, 0.97338901]), 'std_test_score': array([0.00414016, 0.00092852, 0.00092852, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00092852, 0.02966907, 0.02417632, 0.01899582,\n",
      "       0.00092852, 0.00092852, 0.00092852, 0.00092852, 0.0084039 ,\n",
      "       0.00604374, 0.00448063, 0.00362868, 0.00582501, 0.00518609,\n",
      "       0.0061426 , 0.0100451 , 0.00404623, 0.00395294, 0.00395051,\n",
      "       0.00581337, 0.00517836, 0.00531556]), 'rank_test_score': array([28, 18, 18, 18, 18, 18, 18, 15, 14, 17, 18, 18, 18, 18, 12, 10,  8,\n",
      "        4,  2,  2,  1, 16, 13,  9, 11,  6,  7,  5])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.904 (+/-0.008) for {'max_depth': 1, 'n_estimators': 1}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 3}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 10}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 30}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 100}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 300}\n",
      "0.906 (+/-0.002) for {'max_depth': 1, 'n_estimators': 1000}\n",
      "0.962 (+/-0.059) for {'max_depth': 3, 'n_estimators': 1}\n",
      "0.965 (+/-0.048) for {'max_depth': 3, 'n_estimators': 3}\n",
      "0.922 (+/-0.038) for {'max_depth': 3, 'n_estimators': 10}\n",
      "0.906 (+/-0.002) for {'max_depth': 3, 'n_estimators': 30}\n",
      "0.906 (+/-0.002) for {'max_depth': 3, 'n_estimators': 100}\n",
      "0.906 (+/-0.002) for {'max_depth': 3, 'n_estimators': 300}\n",
      "0.906 (+/-0.002) for {'max_depth': 3, 'n_estimators': 1000}\n",
      "0.969 (+/-0.017) for {'max_depth': 10, 'n_estimators': 1}\n",
      "0.972 (+/-0.012) for {'max_depth': 10, 'n_estimators': 3}\n",
      "0.972 (+/-0.009) for {'max_depth': 10, 'n_estimators': 10}\n",
      "0.975 (+/-0.007) for {'max_depth': 10, 'n_estimators': 30}\n",
      "0.975 (+/-0.012) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.975 (+/-0.01) for {'max_depth': 10, 'n_estimators': 300}\n",
      "0.976 (+/-0.012) for {'max_depth': 10, 'n_estimators': 1000}\n",
      "0.956 (+/-0.02) for {'max_depth': None, 'n_estimators': 1}\n",
      "0.966 (+/-0.008) for {'max_depth': None, 'n_estimators': 3}\n",
      "0.972 (+/-0.008) for {'max_depth': None, 'n_estimators': 10}\n",
      "0.972 (+/-0.008) for {'max_depth': None, 'n_estimators': 30}\n",
      "0.973 (+/-0.012) for {'max_depth': None, 'n_estimators': 100}\n",
      "0.973 (+/-0.01) for {'max_depth': None, 'n_estimators': 300}\n",
      "0.973 (+/-0.011) for {'max_depth': None, 'n_estimators': 1000}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.976\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'max_depth': 10, 'n_estimators': 1000}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86        55\n",
      "           1       0.98      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.93      0.92      0.92       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 47   8]\n",
      " [  7 455]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"n_estimators\": [1, 3, 10, 30, 100, 300, 1000], \"max_depth\": [1, 3, 10, None]}]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = GridSearchCV(RandomForestClassifier(random_state=0), param_grid, cv=cv)\n",
    "rfc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, rfc)\n",
    "summary[\"Random Forest\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914bcde-afdb-4ea0-93a5-4cc2fe2f9d39",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da015f64-7132-4a20-938c-4de52f430753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "     --------------------------------------- 99.8/99.8 MB 14.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\12039\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\12039\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from xgboost) (1.9.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12a311c8-7eec-4795-9a42-9c30d3355ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.09974146, 0.10755525, 0.09180932, 0.09690866, 0.10032735,\n",
      "       0.09130292, 0.07766566, 0.06495523, 0.06257343]), 'std_fit_time': array([0.01252798, 0.01582916, 0.00227681, 0.01334238, 0.01154239,\n",
      "       0.00320648, 0.00792783, 0.0014251 , 0.00102909]), 'mean_score_time': array([0.01296697, 0.01222668, 0.01271505, 0.01221008, 0.01279783,\n",
      "       0.01216421, 0.01244779, 0.01174259, 0.01165152]), 'std_score_time': array([7.26246076e-04, 1.16447385e-03, 4.22682546e-04, 1.24689098e-03,\n",
      "       4.16690474e-04, 5.12546936e-05, 4.44880288e-04, 1.15341167e-03,\n",
      "       1.57848589e-03]), 'param_reg_alpha': masked_array(data=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'reg_alpha': 0.001}, {'reg_alpha': 0.003}, {'reg_alpha': 0.01}, {'reg_alpha': 0.03}, {'reg_alpha': 0.1}, {'reg_alpha': 0.3}, {'reg_alpha': 1}, {'reg_alpha': 3}, {'reg_alpha': 10}], 'split0_test_score': array([0.97101449, 0.97342995, 0.97101449, 0.97342995, 0.97342995,\n",
      "       0.97584541, 0.97826087, 0.98067633, 0.98309179]), 'split1_test_score': array([0.96859903, 0.96859903, 0.96618357, 0.97101449, 0.97101449,\n",
      "       0.97342995, 0.97826087, 0.97826087, 0.97826087]), 'split2_test_score': array([0.98305085, 0.97578692, 0.98547215, 0.98305085, 0.98547215,\n",
      "       0.98305085, 0.98547215, 0.98547215, 0.98547215]), 'split3_test_score': array([0.95883777, 0.95883777, 0.95883777, 0.95883777, 0.95883777,\n",
      "       0.96368039, 0.968523  , 0.96610169, 0.96610169]), 'split4_test_score': array([0.968523  , 0.97094431, 0.968523  , 0.97336562, 0.97094431,\n",
      "       0.97336562, 0.97336562, 0.97336562, 0.97578692]), 'mean_test_score': array([0.97000503, 0.9695196 , 0.9700062 , 0.97193974, 0.97193974,\n",
      "       0.97387444, 0.9767765 , 0.97677533, 0.97774269]), 'std_test_score': array([0.00774717, 0.00585745, 0.00873795, 0.00774891, 0.00847178,\n",
      "       0.00620429, 0.00565288, 0.00661229, 0.00675224]), 'rank_test_score': array([8, 9, 7, 5, 6, 4, 2, 3, 1])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.97 (+/-0.015) for {'reg_alpha': 0.001}\n",
      "0.97 (+/-0.012) for {'reg_alpha': 0.003}\n",
      "0.97 (+/-0.017) for {'reg_alpha': 0.01}\n",
      "0.972 (+/-0.015) for {'reg_alpha': 0.03}\n",
      "0.972 (+/-0.017) for {'reg_alpha': 0.1}\n",
      "0.974 (+/-0.012) for {'reg_alpha': 0.3}\n",
      "0.977 (+/-0.011) for {'reg_alpha': 1}\n",
      "0.977 (+/-0.013) for {'reg_alpha': 3}\n",
      "0.978 (+/-0.014) for {'reg_alpha': 10}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.978\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'reg_alpha': 10}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        55\n",
      "           1       0.99      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.94      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 49   6]\n",
      " [  8 454]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"reg_alpha\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]}]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbc = GridSearchCV(XGBClassifier(random_state=0), param_grid, cv=cv)\n",
    "xgbc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, xgbc)\n",
    "summary[\"XGBoost\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b103c-d597-4344-b7f2-76d6f8d36919",
   "metadata": {},
   "source": [
    "### Linear SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5ada107-7827-44f2-b05c-65a887a59815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.01427002, 0.01955972, 0.01724691, 0.02276049, 0.02452269,\n",
      "       0.0246511 , 0.02480698]), 'std_fit_time': array([0.00664075, 0.00437733, 0.0014037 , 0.00576183, 0.00230959,\n",
      "       0.00155877, 0.00205269]), 'mean_score_time': array([0.00173674, 0.00129595, 0.0016057 , 0.00156651, 0.00152817,\n",
      "       0.00179057, 0.00174813]), 'std_score_time': array([0.00043011, 0.00043136, 0.00050365, 0.00042707, 0.00046257,\n",
      "       0.0004882 , 0.0003647 ]), 'param_C': masked_array(data=[0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
      "             mask=[False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01}, {'C': 0.03}, {'C': 0.1}, {'C': 0.3}, {'C': 1}, {'C': 3}, {'C': 10}], 'split0_test_score': array([0.94927536, 0.96135266, 0.97342995, 0.97826087, 0.9589372 ,\n",
      "       0.97584541, 0.97101449]), 'split1_test_score': array([0.94927536, 0.96618357, 0.97584541, 0.97584541, 0.97342995,\n",
      "       0.96859903, 0.96618357]), 'split2_test_score': array([0.96125908, 0.968523  , 0.97578692, 0.98062954, 0.91283293,\n",
      "       0.97578692, 0.97094431]), 'split3_test_score': array([0.93220339, 0.95157385, 0.95641646, 0.95883777, 0.95641646,\n",
      "       0.95883777, 0.95883777]), 'split4_test_score': array([0.94673123, 0.96610169, 0.97094431, 0.97336562, 0.968523  ,\n",
      "       0.88861985, 0.968523  ]), 'mean_test_score': array([0.94774889, 0.96274696, 0.97048461, 0.97338784, 0.95402791,\n",
      "       0.9535378 , 0.96710063]), 'std_test_score': array([0.00927236, 0.00605326, 0.00726157, 0.00766725, 0.0215099 ,\n",
      "       0.03305124, 0.00449958]), 'rank_test_score': array([7, 4, 2, 1, 5, 6, 3])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.948 (+/-0.019) for {'C': 0.01}\n",
      "0.963 (+/-0.012) for {'C': 0.03}\n",
      "0.97 (+/-0.015) for {'C': 0.1}\n",
      "0.973 (+/-0.015) for {'C': 0.3}\n",
      "0.954 (+/-0.043) for {'C': 1}\n",
      "0.954 (+/-0.066) for {'C': 3}\n",
      "0.967 (+/-0.009) for {'C': 10}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.973\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'C': 0.3}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85        55\n",
      "           1       0.98      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.91      0.92       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 46   9]\n",
      " [  7 455]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973,\n",
       " 'Linear SVMs': 0.969}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"C\": [0.01, 0.03, 0.1, 0.3, 1, 3, 10]}]\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = GridSearchCV(LinearSVC(random_state=0), param_grid, cv=cv)\n",
    "lsvc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, lsvc)\n",
    "summary[\"Linear SVMs\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a821b11-593d-4976-af23-0be8d49ff1b6",
   "metadata": {},
   "source": [
    "### Kernelized SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd4055ca-5169-48b7-bbea-66e42899598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.01743441, 0.01848555, 0.01735725, 0.02168794, 0.02325506,\n",
      "       0.03765216, 0.06712594, 0.07670684, 0.08873959, 0.02312164,\n",
      "       0.02181597, 0.01958241, 0.02189617, 0.03168807, 0.05600424,\n",
      "       0.07693968, 0.09323025, 0.09946046, 0.02374101, 0.02820382,\n",
      "       0.01891904, 0.02474079, 0.04011712, 0.06678138, 0.08585529,\n",
      "       0.08950229, 0.10329452, 0.01478748, 0.0151711 , 0.01209607,\n",
      "       0.01776061, 0.03058472, 0.05708709, 0.07530527, 0.0859818 ,\n",
      "       0.10610332, 0.01025319, 0.01348009, 0.01061773, 0.01630993,\n",
      "       0.02768607, 0.06402831, 0.0760066 , 0.08844728, 0.10542536,\n",
      "       0.00862675, 0.01407671, 0.01012387, 0.01579471, 0.02463427,\n",
      "       0.05506945, 0.07391648, 0.08505993, 0.10523367, 0.00860057,\n",
      "       0.01476412, 0.01396852, 0.01680245, 0.02275476, 0.05316606,\n",
      "       0.08260403, 0.08603396, 0.11348057]), 'std_fit_time': array([0.00306212, 0.00069709, 0.00085565, 0.00397294, 0.0009808 ,\n",
      "       0.0014921 , 0.00514174, 0.00113495, 0.00163614, 0.00641149,\n",
      "       0.00164939, 0.00061296, 0.00111938, 0.00216297, 0.00639211,\n",
      "       0.01281876, 0.0117541 , 0.00391761, 0.01071859, 0.00461133,\n",
      "       0.00191869, 0.00220537, 0.00168258, 0.00720879, 0.00667078,\n",
      "       0.00430862, 0.00263613, 0.00544716, 0.00068693, 0.0012845 ,\n",
      "       0.00067306, 0.00070274, 0.00503233, 0.00080837, 0.00156759,\n",
      "       0.0042409 , 0.00111741, 0.00189188, 0.00085169, 0.00102018,\n",
      "       0.00084344, 0.00217917, 0.00577862, 0.00725583, 0.00378372,\n",
      "       0.00053756, 0.00204548, 0.00089534, 0.00147255, 0.00275571,\n",
      "       0.00761474, 0.00240749, 0.00071743, 0.00393133, 0.000473  ,\n",
      "       0.00145611, 0.00157989, 0.00130224, 0.00162719, 0.00677238,\n",
      "       0.00373861, 0.00553836, 0.01306382]), 'mean_score_time': array([0.00946908, 0.00962763, 0.01025186, 0.01038151, 0.01309891,\n",
      "       0.02031112, 0.03264871, 0.03705544, 0.03854871, 0.01143885,\n",
      "       0.01166368, 0.01097512, 0.01188102, 0.01706595, 0.02985015,\n",
      "       0.03452029, 0.0405632 , 0.04153452, 0.01328545, 0.01199694,\n",
      "       0.00940576, 0.01288533, 0.0191761 , 0.03293366, 0.04737473,\n",
      "       0.03868656, 0.04168358, 0.0078949 , 0.00675478, 0.00588984,\n",
      "       0.00777888, 0.01346927, 0.026162  , 0.03489127, 0.03568511,\n",
      "       0.04398851, 0.00464983, 0.00599623, 0.00454602, 0.00673532,\n",
      "       0.0117579 , 0.02375159, 0.03173585, 0.04039836, 0.04213991,\n",
      "       0.00489569, 0.00572362, 0.00462275, 0.00649495, 0.01021724,\n",
      "       0.02209768, 0.03453803, 0.03664508, 0.04022689, 0.00388641,\n",
      "       0.00505424, 0.00515785, 0.00672379, 0.00997186, 0.02237782,\n",
      "       0.03625374, 0.03718915, 0.04278245]), 'std_score_time': array([0.00074326, 0.0004205 , 0.0007912 , 0.00106932, 0.00083983,\n",
      "       0.00104872, 0.0004631 , 0.00377069, 0.00163261, 0.00087509,\n",
      "       0.00050033, 0.00150919, 0.00087149, 0.00117532, 0.00361226,\n",
      "       0.00341996, 0.00296714, 0.00130088, 0.00699957, 0.00212241,\n",
      "       0.00072543, 0.00194061, 0.00111425, 0.00294224, 0.01858634,\n",
      "       0.00173925, 0.00130774, 0.00066424, 0.0003685 , 0.0006924 ,\n",
      "       0.00041306, 0.0009056 , 0.00092091, 0.00156966, 0.00098008,\n",
      "       0.00298971, 0.0005952 , 0.00058169, 0.00040491, 0.00045701,\n",
      "       0.00030774, 0.0014347 , 0.00434632, 0.00599212, 0.00277238,\n",
      "       0.00134023, 0.00089866, 0.00053083, 0.00099224, 0.00054019,\n",
      "       0.00072104, 0.00203357, 0.00088386, 0.00278193, 0.00039735,\n",
      "       0.00010488, 0.00050527, 0.00080626, 0.00041824, 0.00169856,\n",
      "       0.0023737 , 0.00224392, 0.00296668]), 'param_C': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 10, 10, 10, 10, 10,\n",
      "                   10, 10, 10, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=['scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10,\n",
      "                   'scale', 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01, 'gamma': 'scale'}, {'C': 0.01, 'gamma': 'auto'}, {'C': 0.01, 'gamma': 0.01}, {'C': 0.01, 'gamma': 0.03}, {'C': 0.01, 'gamma': 0.1}, {'C': 0.01, 'gamma': 0.3}, {'C': 0.01, 'gamma': 1}, {'C': 0.01, 'gamma': 3}, {'C': 0.01, 'gamma': 10}, {'C': 0.03, 'gamma': 'scale'}, {'C': 0.03, 'gamma': 'auto'}, {'C': 0.03, 'gamma': 0.01}, {'C': 0.03, 'gamma': 0.03}, {'C': 0.03, 'gamma': 0.1}, {'C': 0.03, 'gamma': 0.3}, {'C': 0.03, 'gamma': 1}, {'C': 0.03, 'gamma': 3}, {'C': 0.03, 'gamma': 10}, {'C': 0.1, 'gamma': 'scale'}, {'C': 0.1, 'gamma': 'auto'}, {'C': 0.1, 'gamma': 0.01}, {'C': 0.1, 'gamma': 0.03}, {'C': 0.1, 'gamma': 0.1}, {'C': 0.1, 'gamma': 0.3}, {'C': 0.1, 'gamma': 1}, {'C': 0.1, 'gamma': 3}, {'C': 0.1, 'gamma': 10}, {'C': 0.3, 'gamma': 'scale'}, {'C': 0.3, 'gamma': 'auto'}, {'C': 0.3, 'gamma': 0.01}, {'C': 0.3, 'gamma': 0.03}, {'C': 0.3, 'gamma': 0.1}, {'C': 0.3, 'gamma': 0.3}, {'C': 0.3, 'gamma': 1}, {'C': 0.3, 'gamma': 3}, {'C': 0.3, 'gamma': 10}, {'C': 1, 'gamma': 'scale'}, {'C': 1, 'gamma': 'auto'}, {'C': 1, 'gamma': 0.01}, {'C': 1, 'gamma': 0.03}, {'C': 1, 'gamma': 0.1}, {'C': 1, 'gamma': 0.3}, {'C': 1, 'gamma': 1}, {'C': 1, 'gamma': 3}, {'C': 1, 'gamma': 10}, {'C': 3, 'gamma': 'scale'}, {'C': 3, 'gamma': 'auto'}, {'C': 3, 'gamma': 0.01}, {'C': 3, 'gamma': 0.03}, {'C': 3, 'gamma': 0.1}, {'C': 3, 'gamma': 0.3}, {'C': 3, 'gamma': 1}, {'C': 3, 'gamma': 3}, {'C': 3, 'gamma': 10}, {'C': 10, 'gamma': 'scale'}, {'C': 10, 'gamma': 'auto'}, {'C': 10, 'gamma': 0.01}, {'C': 10, 'gamma': 0.03}, {'C': 10, 'gamma': 0.1}, {'C': 10, 'gamma': 0.3}, {'C': 10, 'gamma': 1}, {'C': 10, 'gamma': 3}, {'C': 10, 'gamma': 10}], 'split0_test_score': array([0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.93961353,\n",
      "       0.98550725, 0.98550725, 0.98309179, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9589372 , 0.98550725,\n",
      "       0.98550725, 0.98550725, 0.97826087, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.97826087, 0.98550725, 0.98550725,\n",
      "       0.98309179, 0.98550725, 0.94927536, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.98550725, 0.98067633, 0.98550725, 0.98309179,\n",
      "       0.98309179, 0.97342995, 0.90821256, 0.90821256, 0.90821256,\n",
      "       0.98550725, 0.98309179, 0.98067633, 0.98309179, 0.97584541,\n",
      "       0.97101449, 0.90821256, 0.90821256, 0.90821256, 0.98309179,\n",
      "       0.98309179, 0.98309179, 0.97826087, 0.96618357, 0.96859903,\n",
      "       0.90821256, 0.90821256, 0.90821256]), 'split1_test_score': array([0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.9057971 , 0.94444444,\n",
      "       0.97826087, 0.97826087, 0.97826087, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.9057971 , 0.95169082, 0.97826087,\n",
      "       0.97826087, 0.97826087, 0.98067633, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.9057971 , 0.98067633, 0.97826087, 0.97826087,\n",
      "       0.97826087, 0.97826087, 0.95169082, 0.9057971 , 0.9057971 ,\n",
      "       0.9057971 , 0.97826087, 0.97826087, 0.97826087, 0.97826087,\n",
      "       0.97826087, 0.98067633, 0.9057971 , 0.9057971 , 0.9057971 ,\n",
      "       0.97826087, 0.97826087, 0.97826087, 0.97826087, 0.97826087,\n",
      "       0.97584541, 0.90821256, 0.9057971 , 0.9057971 , 0.97826087,\n",
      "       0.97826087, 0.97826087, 0.97826087, 0.96859903, 0.97584541,\n",
      "       0.90821256, 0.9057971 , 0.9057971 ]), 'split2_test_score': array([0.90799031, 0.90799031, 0.90799031, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.90799031, 0.90799031, 0.90799031, 0.94915254,\n",
      "       0.98547215, 0.98547215, 0.98305085, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.90799031, 0.90799031, 0.96125908, 0.98547215,\n",
      "       0.98547215, 0.98547215, 0.98305085, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.90799031, 0.98547215, 0.98547215, 0.98547215,\n",
      "       0.98789346, 0.98789346, 0.95399516, 0.90799031, 0.90799031,\n",
      "       0.90799031, 0.98547215, 0.98789346, 0.98547215, 0.98789346,\n",
      "       0.98789346, 0.98062954, 0.90799031, 0.90799031, 0.90799031,\n",
      "       0.98547215, 0.98789346, 0.98789346, 0.98789346, 0.98547215,\n",
      "       0.97578692, 0.90799031, 0.90799031, 0.90799031, 0.98547215,\n",
      "       0.98789346, 0.98547215, 0.98789346, 0.98305085, 0.97578692,\n",
      "       0.90799031, 0.90799031, 0.90799031]), 'split3_test_score': array([0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.93220339,\n",
      "       0.968523  , 0.96610169, 0.968523  , 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.9346247 , 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.96610169, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.96368039, 0.968523  , 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.94915254, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.96610169, 0.968523  , 0.968523  , 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.90556901, 0.9031477 , 0.9031477 ,\n",
      "       0.968523  , 0.968523  , 0.968523  , 0.968523  , 0.968523  ,\n",
      "       0.96610169, 0.90556901, 0.9031477 , 0.9031477 , 0.968523  ,\n",
      "       0.968523  , 0.968523  , 0.968523  , 0.96610169, 0.96125908,\n",
      "       0.90556901, 0.9031477 , 0.9031477 ]), 'split4_test_score': array([0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.90556901, 0.94188862,\n",
      "       0.96610169, 0.97578692, 0.96610169, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.90556901, 0.94430993, 0.97578692,\n",
      "       0.97578692, 0.97578692, 0.96610169, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.90556901, 0.97336562, 0.97578692, 0.97578692,\n",
      "       0.97578692, 0.97094431, 0.94673123, 0.90556901, 0.90556901,\n",
      "       0.90556901, 0.97578692, 0.97578692, 0.97578692, 0.97578692,\n",
      "       0.97578692, 0.968523  , 0.90799031, 0.90799031, 0.90799031,\n",
      "       0.97578692, 0.97578692, 0.97578692, 0.97578692, 0.97336562,\n",
      "       0.96125908, 0.91041162, 0.90799031, 0.90799031, 0.97578692,\n",
      "       0.97336562, 0.97578692, 0.97336562, 0.97336562, 0.96368039,\n",
      "       0.91041162, 0.90799031, 0.90799031]), 'mean_test_score': array([0.90614451, 0.90614451, 0.90614451, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.90614451, 0.90614451, 0.90614451, 0.9414605 ,\n",
      "       0.97677299, 0.97822578, 0.97580564, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.90614451, 0.90614451, 0.95016434, 0.97871004,\n",
      "       0.97871004, 0.97871004, 0.97483829, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.90614451, 0.97629107, 0.97871004, 0.97871004,\n",
      "       0.97871121, 0.97822578, 0.95016902, 0.90614451, 0.90614451,\n",
      "       0.90614451, 0.97822578, 0.97822812, 0.97871004, 0.97871121,\n",
      "       0.97871121, 0.97435636, 0.90711186, 0.9066276 , 0.9066276 ,\n",
      "       0.97871004, 0.97871121, 0.97822812, 0.97871121, 0.97629341,\n",
      "       0.97000152, 0.90807921, 0.9066276 , 0.9066276 , 0.97822695,\n",
      "       0.97822695, 0.97822695, 0.97726076, 0.97146015, 0.96903417,\n",
      "       0.90807921, 0.9066276 , 0.9066276 ]), 'std_test_score': array([0.00092852, 0.00092852, 0.00092852, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00092852, 0.00092852, 0.00092852, 0.00560999,\n",
      "       0.00819897, 0.00718989, 0.00719462, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00092852, 0.00092852, 0.00977897, 0.00639453,\n",
      "       0.00639453, 0.00639453, 0.00729245, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00092852, 0.00741607, 0.00639453, 0.00639453,\n",
      "       0.00657354, 0.00766353, 0.00247408, 0.00092852, 0.00092852,\n",
      "       0.00092852, 0.00718989, 0.00631763, 0.00639453, 0.00657354,\n",
      "       0.00657354, 0.00544443, 0.00117166, 0.00195067, 0.00195067,\n",
      "       0.00639453, 0.00657354, 0.00631763, 0.00657354, 0.00560843,\n",
      "       0.00566193, 0.00153523, 0.00195067, 0.00195067, 0.0059377 ,\n",
      "       0.00685432, 0.0059377 , 0.00642768, 0.00636736, 0.00602158,\n",
      "       0.00153523, 0.00195067, 0.00195067]), 'rank_test_score': array([43, 43, 43, 43, 43, 43, 43, 43, 43, 33, 22, 18, 25, 43, 43, 43, 43,\n",
      "       43, 32,  6,  6,  6, 26, 43, 43, 43, 43, 24,  6,  6,  1, 18, 31, 43,\n",
      "       43, 43, 18, 13,  6,  1,  1, 27, 36, 37, 37,  6,  1, 13,  1, 23, 29,\n",
      "       34, 37, 37, 15, 15, 15, 21, 28, 30, 34, 37, 37])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 'scale'}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 'auto'}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 0.01}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 0.03}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 0.1}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 0.3}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 1}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 3}\n",
      "0.906 (+/-0.002) for {'C': 0.01, 'gamma': 10}\n",
      "0.941 (+/-0.011) for {'C': 0.03, 'gamma': 'scale'}\n",
      "0.977 (+/-0.016) for {'C': 0.03, 'gamma': 'auto'}\n",
      "0.978 (+/-0.014) for {'C': 0.03, 'gamma': 0.01}\n",
      "0.976 (+/-0.014) for {'C': 0.03, 'gamma': 0.03}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 0.1}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 0.3}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 1}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 3}\n",
      "0.906 (+/-0.002) for {'C': 0.03, 'gamma': 10}\n",
      "0.95 (+/-0.02) for {'C': 0.1, 'gamma': 'scale'}\n",
      "0.979 (+/-0.013) for {'C': 0.1, 'gamma': 'auto'}\n",
      "0.979 (+/-0.013) for {'C': 0.1, 'gamma': 0.01}\n",
      "0.979 (+/-0.013) for {'C': 0.1, 'gamma': 0.03}\n",
      "0.975 (+/-0.015) for {'C': 0.1, 'gamma': 0.1}\n",
      "0.906 (+/-0.002) for {'C': 0.1, 'gamma': 0.3}\n",
      "0.906 (+/-0.002) for {'C': 0.1, 'gamma': 1}\n",
      "0.906 (+/-0.002) for {'C': 0.1, 'gamma': 3}\n",
      "0.906 (+/-0.002) for {'C': 0.1, 'gamma': 10}\n",
      "0.976 (+/-0.015) for {'C': 0.3, 'gamma': 'scale'}\n",
      "0.979 (+/-0.013) for {'C': 0.3, 'gamma': 'auto'}\n",
      "0.979 (+/-0.013) for {'C': 0.3, 'gamma': 0.01}\n",
      "0.979 (+/-0.013) for {'C': 0.3, 'gamma': 0.03}\n",
      "0.978 (+/-0.015) for {'C': 0.3, 'gamma': 0.1}\n",
      "0.95 (+/-0.005) for {'C': 0.3, 'gamma': 0.3}\n",
      "0.906 (+/-0.002) for {'C': 0.3, 'gamma': 1}\n",
      "0.906 (+/-0.002) for {'C': 0.3, 'gamma': 3}\n",
      "0.906 (+/-0.002) for {'C': 0.3, 'gamma': 10}\n",
      "0.978 (+/-0.014) for {'C': 1, 'gamma': 'scale'}\n",
      "0.978 (+/-0.013) for {'C': 1, 'gamma': 'auto'}\n",
      "0.979 (+/-0.013) for {'C': 1, 'gamma': 0.01}\n",
      "0.979 (+/-0.013) for {'C': 1, 'gamma': 0.03}\n",
      "0.979 (+/-0.013) for {'C': 1, 'gamma': 0.1}\n",
      "0.974 (+/-0.011) for {'C': 1, 'gamma': 0.3}\n",
      "0.907 (+/-0.002) for {'C': 1, 'gamma': 1}\n",
      "0.907 (+/-0.004) for {'C': 1, 'gamma': 3}\n",
      "0.907 (+/-0.004) for {'C': 1, 'gamma': 10}\n",
      "0.979 (+/-0.013) for {'C': 3, 'gamma': 'scale'}\n",
      "0.979 (+/-0.013) for {'C': 3, 'gamma': 'auto'}\n",
      "0.978 (+/-0.013) for {'C': 3, 'gamma': 0.01}\n",
      "0.979 (+/-0.013) for {'C': 3, 'gamma': 0.03}\n",
      "0.976 (+/-0.011) for {'C': 3, 'gamma': 0.1}\n",
      "0.97 (+/-0.011) for {'C': 3, 'gamma': 0.3}\n",
      "0.908 (+/-0.003) for {'C': 3, 'gamma': 1}\n",
      "0.907 (+/-0.004) for {'C': 3, 'gamma': 3}\n",
      "0.907 (+/-0.004) for {'C': 3, 'gamma': 10}\n",
      "0.978 (+/-0.012) for {'C': 10, 'gamma': 'scale'}\n",
      "0.978 (+/-0.014) for {'C': 10, 'gamma': 'auto'}\n",
      "0.978 (+/-0.012) for {'C': 10, 'gamma': 0.01}\n",
      "0.977 (+/-0.013) for {'C': 10, 'gamma': 0.03}\n",
      "0.971 (+/-0.013) for {'C': 10, 'gamma': 0.1}\n",
      "0.969 (+/-0.012) for {'C': 10, 'gamma': 0.3}\n",
      "0.908 (+/-0.003) for {'C': 10, 'gamma': 1}\n",
      "0.907 (+/-0.004) for {'C': 10, 'gamma': 3}\n",
      "0.907 (+/-0.004) for {'C': 10, 'gamma': 10}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.979\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'C': 0.3, 'gamma': 0.03}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        55\n",
      "           1       0.99      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.92      0.94      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 49   6]\n",
      " [  8 454]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973,\n",
       " 'Linear SVMs': 0.969,\n",
       " 'Kernelized SVMs': 0.973}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"C\": [0.01, 0.03, 0.1, 0.3, 1, 3, 10], \"gamma\": [\"scale\", \"auto\", 0.01, 0.03, 0.1, 0.3, 1, 3, 10]}]\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = GridSearchCV(SVC(random_state=0), param_grid, cv=cv)\n",
    "svc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, svc)\n",
    "summary[\"Kernelized SVMs\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d26db3-bb28-4410-87f2-9bd2a590e399",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb1617a3-db7f-4155-8df8-24123d0532d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parameter estimation results: \n",
      "{'mean_fit_time': array([0.18129659, 0.50103211, 0.57148833, 0.26352835, 0.58625808,\n",
      "       0.54303122, 0.85335183, 1.09590454, 0.94281211]), 'std_fit_time': array([0.01596171, 0.01342196, 0.08961589, 0.00711293, 0.02772268,\n",
      "       0.05128306, 0.03081536, 0.17652465, 0.07955283]), 'mean_score_time': array([0.00169311, 0.00220051, 0.00204635, 0.0020021 , 0.00194721,\n",
      "       0.00186572, 0.00161142, 0.00224957, 0.00207753]), 'std_score_time': array([5.44896753e-04, 3.99404013e-04, 4.91035783e-05, 6.86811056e-06,\n",
      "       2.14353413e-04, 2.55535365e-04, 4.80233268e-04, 3.98739677e-04,\n",
      "       3.40701401e-05]), 'param_hidden_layer_sizes': masked_array(data=[(10,), (10,), (10,), (30,), (30,), (30,), (100,),\n",
      "                   (100,), (100,)],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_solver': masked_array(data=['lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
      "                   'lbfgs', 'sgd', 'adam'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (10,), 'solver': 'lbfgs'}, {'hidden_layer_sizes': (10,), 'solver': 'sgd'}, {'hidden_layer_sizes': (10,), 'solver': 'adam'}, {'hidden_layer_sizes': (30,), 'solver': 'lbfgs'}, {'hidden_layer_sizes': (30,), 'solver': 'sgd'}, {'hidden_layer_sizes': (30,), 'solver': 'adam'}, {'hidden_layer_sizes': (100,), 'solver': 'lbfgs'}, {'hidden_layer_sizes': (100,), 'solver': 'sgd'}, {'hidden_layer_sizes': (100,), 'solver': 'adam'}], 'split0_test_score': array([0.96859903, 0.93961353, 0.97584541, 0.96376812, 0.94202899,\n",
      "       0.96618357, 0.97584541, 0.94202899, 0.97101449]), 'split1_test_score': array([0.97342995, 0.94444444, 0.97101449, 0.97342995, 0.94444444,\n",
      "       0.97101449, 0.96859903, 0.94444444, 0.97342995]), 'split2_test_score': array([0.97820823, 0.95883777, 0.97578692, 0.97578692, 0.95641646,\n",
      "       0.97578692, 0.97820823, 0.94673123, 0.97820823]), 'split3_test_score': array([0.96368039, 0.9346247 , 0.95157385, 0.96610169, 0.93220339,\n",
      "       0.95641646, 0.968523  , 0.93220339, 0.95157385]), 'split4_test_score': array([0.97094431, 0.94188862, 0.968523  , 0.96368039, 0.94430993,\n",
      "       0.97578692, 0.968523  , 0.94188862, 0.96610169]), 'mean_test_score': array([0.97097238, 0.94388181, 0.96854874, 0.96855341, 0.94388064,\n",
      "       0.96903768, 0.97193974, 0.94145933, 0.96806564]), 'std_test_score': array([0.00484157, 0.00814718, 0.00894271, 0.00507466, 0.00771279,\n",
      "       0.00724505, 0.00422035, 0.00495752, 0.00912338]), 'rank_test_score': array([2, 7, 5, 4, 8, 3, 1, 9, 6])}\n",
      "\n",
      "*** Grid scores: \n",
      "0.971 (+/-0.01) for {'hidden_layer_sizes': (10,), 'solver': 'lbfgs'}\n",
      "0.944 (+/-0.016) for {'hidden_layer_sizes': (10,), 'solver': 'sgd'}\n",
      "0.969 (+/-0.018) for {'hidden_layer_sizes': (10,), 'solver': 'adam'}\n",
      "0.969 (+/-0.01) for {'hidden_layer_sizes': (30,), 'solver': 'lbfgs'}\n",
      "0.944 (+/-0.015) for {'hidden_layer_sizes': (30,), 'solver': 'sgd'}\n",
      "0.969 (+/-0.014) for {'hidden_layer_sizes': (30,), 'solver': 'adam'}\n",
      "0.972 (+/-0.008) for {'hidden_layer_sizes': (100,), 'solver': 'lbfgs'}\n",
      "0.941 (+/-0.01) for {'hidden_layer_sizes': (100,), 'solver': 'sgd'}\n",
      "0.968 (+/-0.018) for {'hidden_layer_sizes': (100,), 'solver': 'adam'}\n",
      "\n",
      "*** Highest accuracy score: \n",
      "0.972\n",
      "\n",
      "*** Best parameters set found: \n",
      "{'hidden_layer_sizes': (100,), 'solver': 'lbfgs'}\n",
      "\n",
      "*** Classification report for the best parameters set: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        55\n",
      "           1       0.98      0.98      0.98       462\n",
      "\n",
      "    accuracy                           0.97       517\n",
      "   macro avg       0.93      0.93      0.93       517\n",
      "weighted avg       0.97      0.97      0.97       517\n",
      "\n",
      "\n",
      "*** Confusion matrix for the best parameters set: \n",
      "[[ 48   7]\n",
      " [  7 455]]\n",
      "\n",
      "*** Final accuracy score: \n",
      "0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12039\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973,\n",
       " 'Linear SVMs': 0.969,\n",
       " 'Kernelized SVMs': 0.973,\n",
       " 'Neural Networks': 0.973}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\"hidden_layer_sizes\": [(10,), (30,), (100,)], \"solver\": [\"lbfgs\", \"sgd\", \"adam\"]}]\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = GridSearchCV(MLPClassifier(random_state=0), param_grid, cv=cv)\n",
    "mlpc, score = train_test2(X_train, X_test, y_train, y_test, param_grid, mlpc)\n",
    "summary[\"Neural Networks\"] = score\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c7782-820d-424d-baa5-ea39189655ec",
   "metadata": {},
   "source": [
    "## Compare and choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bcb22f4-04cf-4207-85dd-1d1fe6ec11b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k-NNs': 0.973,\n",
       " 'Logistic Regression': 0.973,\n",
       " 'Decision Trees': 0.973,\n",
       " 'Random Forest': 0.971,\n",
       " 'XGBoost': 0.973,\n",
       " 'Linear SVMs': 0.969,\n",
       " 'Kernelized SVMs': 0.973,\n",
       " 'Neural Networks': 0.973}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b2a9d-2eb9-482c-a412-a16bbe211bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
